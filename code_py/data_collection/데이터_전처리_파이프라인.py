#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().system('jupyter nbconvert --to script ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸.ipynb')


# In[ ]:


pd.set_option('display.max_columns', None)
# pd.set_option('display.width', None)
# pd.set_option('display.expand_frame_repr', False)

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

import os
import pandas as pd
import numpy as np

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
test = pd.read_csv('./test/test.csv', encoding='CP949')
df23 = pd.read_csv('./data/train_subway23.csv', encoding='CP949')
df22 = pd.read_csv('./data/train_subway22.csv', encoding='CP949')
df21 = pd.read_csv('./data/train_subway21.csv', encoding='CP949')

# 2021, 2022, ,2023ë…„ ë°ì´í„° í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©
df = pd.concat([df21, df22, df23], axis=0, ignore_index=True)

# í™˜ìŠ¹ì—­ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
t = pd.read_excel('./data/í™˜ìŠ¹ì—­.xlsx', names =['Line','station_name','transfer'], header=0)

# ì§€í•˜ì² ì—­ ì£¼ì†Œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
address = pd.read_csv('./data/result_address.csv', encoding='CP949')

# address ë°ì´í„°ì— ìŠ¤í¬ë˜í•‘ì—ì„œ ë¹ ì§„ ì£¼ì†Œ ì¶”ê°€í•´ì„œ ë³‘í•©
subway_13 = pd.DataFrame({'ì—­ëª…':['ì„±ìˆ˜E', 'ì‘ì•”S','ë¶ˆì•”ì‚°']
             ,'ì£¼ì†Œ':['ì„œìš¸ ì„±ë™êµ¬ ì•„ì°¨ì‚°ë¡œ 100','ì„œìš¸ ì€í‰êµ¬ ì¦ì‚°ë¡œ 477','ì„œìš¸ ë…¸ì›êµ¬ ìƒê³„ë¡œ 305']})

address = pd.concat([address, subway_13], axis=0).reset_index(drop=True)

df.shape

- address ì •í•©ì„±ì„ ìœ„í•´ ì „ì²˜ë¦¬

1. ì—­ ì´ë¦„ì—ì„œ ê´„í˜¸ ì œê±°í•´ì„œ ì´ë¦„ í†µì¼  
ì—­ ì´ë¦„ì—ì„œ ê´„í˜¸ ë° ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±°  
ì˜ˆ: 'ê°•ë‚¨(2í˜¸ì„ )' â†’ 'ê°•ë‚¨'

2. ì§€í•˜ì² ì—­ ì£¼ì†ŒëŠ” ã…‡ã…‡êµ¬, ì¸ì²œ, ê²½ê¸°ë¡œ í†µì¼  
'ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ...' â†’ 'ê°•ë‚¨êµ¬'  
'ê²½ê¸°ë„ ìˆ˜ì›ì‹œ ...' â†’ 'ê²½ê¸°ë„'  
  
3. ì¸ì²œ, ê²½ê¸° ì£¼ì†Œ ì²˜ë¦¬  
ì¸ì²œ, ê²½ê¸°ì— ì†í•˜ëŠ” ì§€í•˜ì² ì—­ì€ ì¸ì²œ, ê²½ê¸°ë¡œ ì£¼ì†Œê°’ í†µì¼  
ê·¸ ì™¸ëŠ” ì›ë˜ ê°’ ìœ ì§€

address.columns=['station_name','address']
address.station_name = address.station_name.apply(lambda x: x.split('(')[0].strip() if '(' in x else x)
address.address = address.address.apply(lambda x: x.split()[0] if 'ì„œìš¸' not in x else x.split()[1])
addr = address['address']  
address['address'] = np.where(addr.str.contains('ì¸ì²œ'), 'ì¸ì²œ',np.where(addr.str.contains('ê²½ê¸°'), 'ê²½ê¸°', addr))

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- **ëŒ€íšŒ ê³µì§€ì‚¬í•­ì— ë”°ë¼ df ì´ìƒì¹˜ ë“œë¡­**
    - <ê³µì§€ì‚¬í•­>
    - ë‚¨ìœ„ë¡€ë¥¼ ì œì™¸í•œ í•œëŒ€ì•~ì˜¤ì´ë„ì—­ êµ¬ê°„ì€ ë‚´ë¶€ í”„ë¡œê·¸ë¨ ì˜¤ë¥˜ë¡œ ì¸í•˜ì—¬ 22ë…„ 6ì›” 13ì¼ê¹Œì§€ 4í˜¸ì„  ì¬ì°¨ì¸ì›ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. 
    - í•´ë‹¹ê¸°ê°„ë™ì•ˆ í•œëŒ€ì•~ì˜¤ì´ë„ì—­ì„ ì´ìš©í•˜ëŠ” ì¸ì›ì€ ëª¨ë‘ ìˆ˜ì¸ë¶„ë‹¹ì„ ì„ ì´ìš©í•˜ëŠ”ê²ƒìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.
    - ë‚¨ìœ„ë¡€ì—­ì€ 21ë…„ 12ì›” 18ì¼ì— ê°œí†µí•˜ì˜€ìœ¼ë©°, í”„ë¡œê·¸ë¨ ë‚´ë¶€ì— ê°œí†µì‚¬í•­ ë°˜ì˜ì´ ëŠ¦ì–´ì ¸ í˜¼ì¡ë„ê°€ 0ìœ¼ë¡œ ì‚°ì¶œëœ ê²ƒìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤.

import os
import numpy as np
import pandas as pd
from pandas.api.types import CategoricalDtype
from holidayskr import year_holidays
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
import joblib
import warnings

warnings.filterwarnings('ignore')

def preprocessing(data, t, address, is_train=True, known_stations=None):
    data = data.copy()
    # 1)  â†’ datetime
    data['TM'] = data['TM'].astype(str)
    data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H')

    # 2) ë²”ì£¼í˜• ë³€í™˜
    cat_columns = ['Line', 'station_number', 'STN', 'station_name', 'Direction']
    for col in cat_columns:
        data[col] = data[col].astype('category')

    # 3) ê²°ì¸¡ê°’ placeholder
    data['WD']     = data['WD'].where(data['WD'] >=   0, np.nan)
    data['WS']     = data['WS'].replace(-99.0,          np.nan)
    data['RN_DAY'] = data['RN_DAY'].replace(-99.0,      np.nan)
    data['RN_HR1'] = data['RN_HR1'].replace(-99.0,      np.nan)
    data['TA']     = data['TA'].replace(-99.0,          np.nan)
    data['ta_chi'] = data['ta_chi'].replace(-99.0,      np.nan)
    data['SI']     = data['SI'].replace(-99.0,          np.nan)
    data['HM']     = data['HM'].replace(-99.0,          np.nan)
    data = data.drop(columns='SI', axis=1)
    
    # 5) station_name êµì •
    data['station_name'] = data['station_name'].astype(str).replace({
        'ë‹¹ê³ ê°œ': 'ë¶ˆì•”ì‚°',
        'ìì–‘(ëšì„¬í•œê°•ê³µì›)': 'ìì–‘',
        'ì‹ ì´Œ(ì§€í•˜)': 'ì‹ ì´Œ'
    })
    # 6) ì‹ ì„¤ì—­ ë³€ìˆ˜ ìƒì„±
    new_station_list = {'êµ¬ë¦¬', 'ë‹¤ì‚°', 'ë™êµ¬ë¦‰', 'ë³„ë‚´', 'ì•”ì‚¬ì—­ì‚¬ê³µì›', 'ì¥ìí˜¸ìˆ˜ê³µì›'}
    known_stations = data['STN'].unique()
    
    # 7) ì‹ ê·œê´€ì¸¡ì†Œ ë³€ìˆ˜
    if is_train:
        # í•™ìŠµ ë°ì´í„°ëŠ” ê¸°ì¤€ì´ ë  known_stationsë¥¼ ë§Œë“¤ì–´ ë‘ê³ 
        data['ì‹ ì„¤ì—­'] = 0
        data['ì‹ ê·œê´€ì¸¡ì†Œ'] = 0
        ì´ìƒì¹˜_4í˜¸ì„ _ì—­ëª… = ['í•œëŒ€ì•','ì¤‘ì•™','ê³ ì”','ì´ˆì§€','ì•ˆì‚°','ì‹ ê¸¸ì˜¨ì²œ','ì •ì™•','ì˜¤ì´ë„','ê°œë´‰'] # ê°œë´‰ì€ ê·¸ëƒ¥ ì—¬ê¸° ì¶”ê°€í•´ì„œ ì²˜ë¦¬í•¨
        ì´ìƒì¹˜_8í˜¸ì„ _ì—­ëª… = ['ë‚¨ìœ„ë¡€']
        
        pattern_8 = '|'.join(ì´ìƒì¹˜_8í˜¸ì„ _ì—­ëª…)
        pattern_4 = '|'.join(ì´ìƒì¹˜_4í˜¸ì„ _ì—­ëª…)
        
        mask_8 = (data['Line'] == 8) & data['station_name'].str.contains(pattern_8) & (data['TM'] < '2022-12-31')
        mask_4 = (data['Line'] == 4) & data['station_name'].str.contains(pattern_4) & (data['TM'] <= '2022-12-31')
        print('trainì…‹ ê³µì§€ì‚¬í•­ ì´ìƒì¹˜ ',(data.shape[0] - data[~(mask_8 | mask_4)].reset_index(drop=True).shape[0])/data.shape[0],'% ì œê±°')
        data = data[~(mask_8 | mask_4)].reset_index(drop=True)

    else:
        # testì—ì„œëŠ” trainì—ì„œ ë„˜ì–´ì˜¨ known_stationsë¥¼ ì´ìš©í•´ í”Œë˜ê·¸ ì²˜ë¦¬
        data['ì‹ ì„¤ì—­'] = data['station_name'].apply(lambda x: 0 if x in known_stations else 1)
        data['ì‹ ê·œê´€ì¸¡ì†Œ'] = data['STN'].apply(lambda x: 0 if x in known_stations else 1)

    # 7 ) ì„ í˜• ë³´ê°„
    cols_to_interp = ['TA', 'WD', 'WS', 'RN_DAY', 'RN_HR1', 'ta_chi','HM']
    data[cols_to_interp] = data[cols_to_interp].interpolate(method='linear', limit_direction='both')
    data[cols_to_interp] = data[cols_to_interp].interpolate(method='linear', limit_direction='both')
    
    # 8) ì™¸ë¶€ í…Œì´ë¸” ë³‘í•©
    data = data.merge(t, on=['Line','station_name'], how='left')
    data['transfer'] = data['transfer'].fillna(0).astype(int)
    data = data.merge(address, on=['station_name'], how='left')

    # 9) íŒŒìƒ ë³€ìˆ˜
    data['year']         = data['TM'].dt.year - 2021
    data['month']        = data['TM'].dt.month
    data['day']          = data['TM'].dt.day
    data['hour']         = data['TM'].dt.hour
    data['weekday']      = data['TM'].dt.dayofweek
    data['week_of_month']= (data['day'] - 1) // 7 + 1
    data['week_of_year'] = data['TM'].dt.isocalendar().week.astype(int)
    data['day_of_year']  = data['TM'].dt.dayofyear

    # 10) ê³µíœ´ì¼ í”Œë˜ê·¸
    holidays = []
    for yr in [2021,2022,2023,2024]:
        holidays += [d for d,_ in year_holidays(yr)]
    data['is_holiday']            = data['TM'].dt.date.isin(holidays).astype(int)
    data['is_day_before_holiday'] = data['TM'].dt.date.shift(-1).isin(holidays).astype(int)
    data['is_day_after_holiday']  = data['TM'].dt.date.shift(1).isin(holidays).astype(int)

    # 11) ì£¼ë§ í”Œë˜ê·¸
    data['is_weekend'] = data['weekday'].isin([5,6]).astype(int)

    # 12) ì‹œê°„ëŒ€ ë²”ì£¼
    data['time_period'] = np.where(data['hour'].isin([7,8,9]), 'ì¶œê·¼',
                             np.where(data['hour'].isin([17,18,19]), 'í‡´ê·¼',
                             np.where((data['hour']>9)&(data['hour']<17), 'ë‚®',
                             np.where((data['hour']>19)&(data['hour']<21), 'ì €ë…',
                             'ë°¤'))))
    direction_order   = ['ìƒì„ ','í•˜ì„ ','ì™¸ì„ ','ë‚´ì„ ']
    time_period_order = ['ë°¤','ì¶œê·¼','ë‚®','ì €ë…','í‡´ê·¼']
    data['Direction']   = data['Direction'].astype(
        CategoricalDtype(categories=direction_order, ordered=True)
    ).cat.codes
    data['time_period'] = data['time_period'].astype(
        CategoricalDtype(categories=time_period_order, ordered=True)
    ).cat.codes

    # 13) ì£¼ê¸°ì„± sin/cos (24h, 7d, 31d, 5w, 52w, 365d)
    data['sin_hod'] = np.sin(2*np.pi * data['hour']        / 24)
    data['cos_hod'] = np.cos(2*np.pi * data['hour']        / 24)
    data['sin_dow'] = np.sin(2*np.pi * data['weekday']     / 7)
    data['cos_dow'] = np.cos(2*np.pi * data['weekday']     / 7)
    data['sin_dom'] = np.sin(2*np.pi * data['day']         / 31)
    data['cos_dom'] = np.cos(2*np.pi * data['day']         / 31)
    data['sin_wom'] = np.sin(2*np.pi * data['week_of_month'] / 5)
    data['cos_wom'] = np.cos(2*np.pi * data['week_of_month'] / 5)
    data['sin_woy'] = np.sin(2*np.pi * data['week_of_year']  / 52)
    data['cos_woy'] = np.cos(2*np.pi * data['week_of_year']  / 52)
    data['sin_doy'] = np.sin(2*np.pi * data['day_of_year']   / 365)
    data['cos_doy'] = np.cos(2*np.pi * data['day_of_year']   / 365)

    
    return data

# ì „ì²˜ë¦¬
df   = preprocessing(df,   t, address, is_train=True)

test = preprocessing(test, t, address,
                               is_train=False,
                               known_stations=known_stations)

print('ì „ì²˜ë¦¬ ì™„ë£Œ')

df.to_parquet('data.parquet')
test.to_parquet('test.parquet')

import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import joblib
import os

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
results = []
final_results = []

model_dir = './models'
os.makedirs(model_dir, exist_ok=True)

for line in sorted(df['Line'].unique()):
    #if line!=8: continue
    print(f"\nğŸ“˜ [Line {line}] ëª¨ë¸ í•™ìŠµ ì‹œì‘")

    df_line = df[df['Line'] == line].copy()
    test_line = test[test['Line'] == line].copy()

    # ë²”ì£¼í˜• ì²˜ë¦¬
    for col in ['Line', 'STN','address']:
        df_line[col] = df_line[col].astype('category')
        test_line[col] = test_line[col].astype('category')

    # âœ… ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (í•µì‹¬)
    df_line = df_line.sort_values('TM')

    # feature & target ì¶”ì¶œ
    X = df_line[feature_cols]
    y = df_line[target_col].astype(int)
    X_test = test_line[feature_cols]

    # ì¸ì½”ë”©
    # ì›-í•« ì¸ì½”ë”©
    X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=False, prefix=cat_cols)
    X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=False, prefix=cat_cols)

    # âœ… ì»¬ëŸ¼ ì •ë ¬ ë° ëˆ„ë½ëœ ì»¬ëŸ¼ ì±„ì›€
    X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]
    X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]
    X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)
    
    # ì •ê·œí™”
    mm = MinMaxScaler()
    X_scaled = mm.fit_transform(X_enc)
    X_test_scaled = mm.transform(X_test_enc)

    # âœ… ì‹œê°„ ìˆœ ë¶„í•  (train:val = 8:2)
    split_idx = int(len(X_scaled) * 0.8)
    X_train = X_scaled[:split_idx]
    X_val   = X_scaled[split_idx:]
    y_train = y.values[:split_idx]
    y_val   = y.values[split_idx:]

    # ëª¨ë¸ ì •ì˜
    model = xgb.XGBRegressor(
        n_estimators=1500,
        learning_rate=0.01,
        max_depth=12,
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_weight=1,
        gamma=0.5,
        reg_alpha=0.3,
        reg_lambda=0.8,
        tree_method='hist',
        early_stopping_rounds=100,
        random_state=42
    )

    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)

    # ì„±ëŠ¥ í‰ê°€
    val_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    r2 = r2_score(y_val, val_pred)
    print(f"âœ… RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")
    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})

    # ì˜ˆì¸¡
    y_pred = np.round(model.predict(X_test_scaled)).astype(int)
    y_pred = np.where(y_pred < 0, 0, y_pred)
    temp = test_line[['hour', 'Line', 'station_number']].copy()
    temp['ì˜ˆì¸¡í˜¼ì¡ë„'] = y_pred
    final_results.append(temp)

    # ëª¨ë¸ ì €ì¥
    model_path = f"./models/xgb_line{line}.pkl"
    joblib.dump(model, model_path)

# ğŸ” ê²°ê³¼ í†µí•© ë° ì €ì¥
final_df = pd.concat(final_results)
output_df = final_df[['ì˜ˆì¸¡í˜¼ì¡ë„']].rename(columns={'ì˜ˆì¸¡í˜¼ì¡ë„': 'Congestion'})

output_df.to_csv(
    './test/250206-2.csv',
    index=False,
    encoding='utf-8'
)

# ğŸ“Š ì„±ëŠ¥ ìš”ì•½
results_df = pd.DataFrame(results)
print(results_df)

gap = pd.read_csv('./test/250206-2.csv') # ë‚´ ë°í„°í„°
gap.shape

import os
ì œì¶œ = pd.read_csv('./test/minjeong.csv') # ë¯¼ì •ì–¸ë‹ˆ ë°ì´í„°

import numpy as np

# ë§ˆìŠ¤í¬ ìƒì„±
from sklearn.metrics import root_mean_squared_error
rmse = mean_squared_error(
    ì œì¶œ['Congestion'],
    gap['Congestion'],
    squared=False      # squared=False í•˜ë©´ RMSE ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¤Œ
)

print(f"RMSE: {rmse:.4f}")

import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import joblib
import os

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
results = []
final_results = []

model_dir = './models'
os.makedirs(model_dir, exist_ok=True)

for line in sorted(df['Line'].unique()):
    #if line!=8: continue
    print(f"\nğŸ“˜ [Line {line}] ëª¨ë¸ í•™ìŠµ ì‹œì‘")

    df_line = df[df['Line'] == line].copy()
    test_line = test[test['Line'] == line].copy()

    # ë²”ì£¼í˜• ì²˜ë¦¬
    for col in ['Line', 'STN','address']:
        df_line[col] = df_line[col].astype('category')
        test_line[col] = test_line[col].astype('category')

    # âœ… ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (í•µì‹¬)
    df_line = df_line.sort_values('TM')

    # feature & target ì¶”ì¶œ
    X = df_line[feature_cols]
    y = df_line[target_col].astype(int)
    X_test = test_line[feature_cols]

    # ì¸ì½”ë”©
    # ì›-í•« ì¸ì½”ë”©
    X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=False)
    X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=False)

    # âœ… ì»¬ëŸ¼ ì •ë ¬ ë° ëˆ„ë½ëœ ì»¬ëŸ¼ ì±„ì›€
    X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]
    X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]
    X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)
    
    # ì •ê·œí™”
    mm = MinMaxScaler()
    X_scaled = mm.fit_transform(X_enc)
    X_test_scaled = mm.transform(X_test_enc)

    # âœ… ì‹œê°„ ìˆœ ë¶„í•  (train:val = 8:2)
    split_idx = int(len(X_scaled) * 0.8)
    X_train = X_scaled[:split_idx]
    X_val   = X_scaled[split_idx:]
    y_train = y.values[:split_idx]
    y_val   = y.values[split_idx:]

    # ëª¨ë¸ ì •ì˜
    model = xgb.XGBRegressor(
        n_estimators=1500,
        learning_rate=0.01,
        max_depth=12,
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_weight=1,
        gamma=0.5,
        reg_alpha=0.3,
        reg_lambda=0.8,
        tree_method='hist',
        early_stopping_rounds=100,
        random_state=42
    )

    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)

    # ì„±ëŠ¥ í‰ê°€
    val_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    r2 = r2_score(y_val, val_pred)
    print(f"âœ… RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")
    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})

    # ì˜ˆì¸¡
    y_pred = np.round(model.predict(X_test_scaled)).astype(int)
    y_pred = np.where(y_pred < 0, 0, y_pred)
    temp = test_line[['hour', 'Line', 'station_number']].copy()
    temp['ì˜ˆì¸¡í˜¼ì¡ë„'] = y_pred
    final_results.append(temp)

    # ëª¨ë¸ ì €ì¥
    model_path = f"./models/xgb_line{line}.pkl"
    joblib.dump(model, model_path)

# ğŸ” ê²°ê³¼ í†µí•© ë° ì €ì¥
final_df = pd.concat(final_results)
output_df = final_df[['ì˜ˆì¸¡í˜¼ì¡ë„']].rename(columns={'ì˜ˆì¸¡í˜¼ì¡ë„': 'Congestion'})

output_df.to_csv(
    './test/250206-2.csv',
    index=False,
    encoding='utf-8'
)

# ğŸ“Š ì„±ëŠ¥ ìš”ì•½
results_df = pd.DataFrame(results)
print(results_df)

get_ipython().system('[image.png](attachment:c00437b1-3e0f-4751-a41c-3aa9ac86f320.png)![image.png](attachment:4064ece4-c190-41d0-a10c-6170728e926e.png)')

gap = pd.read_csv('./test/250206.csv') # ë‚´ ë°í„°í„°
gap.shape

import os
ì œì¶œ = pd.read_csv('./test/250206-1.csv') # ë¯¼ì •ì–¸ë‹ˆ ë°ì´í„°

import numpy as np

# ë§ˆìŠ¤í¬ ìƒì„±
from sklearn.metrics import root_mean_squared_error
rmse = mean_squared_error(
    ì œì¶œ['Congestion'],
    gap['Congestion'],
    squared=False      # squared=False í•˜ë©´ RMSE ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¤Œ
)

print(f"RMSE: {rmse:.4f}")

# # 2) í”„ë¡œíŒŒì¼ ë¦¬í¬íŠ¸ ìƒì„±
# from pycaret.regression import *
# from ydata_profiling import ProfileReport

# profile = ProfileReport(
#     df,
#     title="My Data Profiling Report",  # ë¦¬í¬íŠ¸ ì œëª©
#     explorative=True,                  # ìì„¸í•œ ë¶„ì„ ëª¨ë“œ
#     minimal=False                       # ìµœì†Œ ë¦¬í¬íŠ¸ ëª¨ë“œ í•´ì œ
# )

# # 3) ê²°ê³¼ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥
# profile.to_file("data_report.html")

from pycaret.regression import (
    setup, compare_models, tune_model,
    finalize_model, predict_model, save_model
)

os.makedirs('./models_pycaret', exist_ok=True)
final_preds = []

for line in sorted(df['Line'].unique()):
    if line!=7: continue 
    print(f"\nâ–¶â–¶ Line {line} AutoML ì‹œì‘")

    # 4-1) Line subset
    train_line = df[df['Line']==line].copy()
    test_line  = test[test['Line']==line].copy()
    train_line['Congestion'] = train_line['Congestion'].astype(int)
    # 4-2) PyCaret setup
    exp = setup(
        data=train_line,
        target='Congestion',
        session_id=42,
        verbose=1
    )

    # 4-3) ëª¨ë¸ ë¹„êµ & ì„ íƒ
    best      = compare_models(verbose=1)           # ê¸°ë³¸ êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
    compare_results = pull()             # compare_models ê²°ê³¼ DataFrame
    print(compare_results)
    save_model(best, f'./models_pycaret/pycaret_base_line{line}')
    
    tuned     = tune_model(best, verbose=1)           # ì„ íƒëœ ëª¨ë¸ í•˜ì´í¼íŠœë‹
    tune_results = pull()                # tune_model ê²°ê³¼ DataFrame
    print(tune_results)
    final_mod = finalize_model(tuned)      # íŠœë‹ëœ ëª¨ë¸ íŒŒì´ë„ë¼ì´ì¦ˆ
    
    # 4-4) í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡
    preds = predict_model(final_mod, data=test_line)
    print(pull())
    # PyCaret íšŒê·€ì˜ ê²½ìš° ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì»¬ëŸ¼ëª… 'Label'ì— ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
    preds = preds.rename(columns={'Label':'Congestion_pred'}) \
                 [['hour','Line','station_number','Congestion_pred']]
    preds['Congestion_pred'] = preds['Congestion_pred'].astype(int)
    final_preds.append(preds)
    
    # 4-5) ëª¨ë¸ ì €ì¥
    save_model(final_mod, f'./models_pycaret/pycaret_tuned_line{line}')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) ê²°ê³¼ í†µí•© ë° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
submission = pd.concat(final_preds).reset_index(drop=True)
submission = submission[['hour','Line','station_number','Congestion_pred']] \
             .rename(columns={'Congestion_pred':'Congestion'})

submission.to_csv('./test/250206_pycaret_submission.csv',
                  index=False, encoding='utf-8')

print("\nâœ… AutoML ì™„ë£Œ, ì œì¶œ íŒŒì¼ ìƒì„±: ./test/250206_pycaret_submission.csv")

import numpy as np
import pandas as pd
from pandas.api.types import CategoricalDtype
from holidayskr import year_holidays

def preprocessing(data, t, address):
    # 1) TM â†’ datetime, ì •ë ¬
    data['TM'] = data['TM'].astype(str)
    data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H')

    # 2) ë²”ì£¼í˜• ë³€í™˜
    cat_columns = ['Line', 'station_number', 'STN', 'station_name', 'Direction']
    for col in cat_columns:
        data[col] = data[col].astype('category')

    # 3) ê²°ì¸¡ê°’ ì²˜ë¦¬ìš© placeholder
    data['WD'] = data['WD'].where(data['WD'] >= 0, np.nan)
    data['WS'] = data['WS'].replace(-99.0, np.nan)
    data['RN_DAY'] = data['RN_DAY'].replace(-99.0, np.nan)
    data['RN_HR1'] = data['RN_HR1'].replace(-99.0, np.nan)
    data['TA'] = data['TA'].replace(-99.0, np.nan)
    data['ta_chi'] = data['ta_chi'].replace(-99.0, np.nan)
    data['SI'] = data['SI'].replace(-99.0, np.nan)
    data['HM'] = data['HM'].replace(-99.0, np.nan)

    # 4) ì´ì§„ í”Œë˜ê·¸í™”
    data['SI'] = data['SI'].notna().astype(int)

    # 5) station_name ì¼ë¶€ êµì •
    data['station_name'] = data['station_name'].astype(str).replace({
        'ë‹¹ê³ ê°œ': 'ë¶ˆì•”ì‚°',
        'ìì–‘(ëšì„¬í•œê°•ê³µì›)': 'ìì–‘',
        'ì‹ ì´Œ(ì§€í•˜)': 'ì‹ ì´Œ'
    })

    # 6) ì™¸ë¶€ í…Œì´ë¸” ë³‘í•©
    data = pd.merge(data, t, on=['Line','station_name'], how='left')
    data['transfer'] = data['transfer'].fillna(0).astype(int)

    data = pd.merge(data, address, on=['station_name'], how='left')

    # 7) í‚¤, ì‹œê°„ íŒŒìƒ
    data['key']   = data['Line'].astype(str) + '_' + data['station_name'].astype(str) + '_' + data['Direction'].astype(str)
    data['year']  = data['TM'].dt.year - 2021
    data['month'] = data['TM'].dt.month
    data['day']   = data['TM'].dt.day
    data['hour']  = data['TM'].dt.hour
    data['weekday']      = data['TM'].dt.weekday
    data['week_of_month']= (data['TM'].dt.day.sub(1) // 7) + 1
    data['week_of_year'] = data['TM'].dt.isocalendar().week.astype(int)
    data['day_of_year']  = data['TM'].dt.dayofyear

    # 8) ê³µíœ´ì¼ í”Œë˜ê·¸
    holidays = []
    for yr in [2021, 2022, 2023, 2024]:
        holidays += [d for d, _ in year_holidays(yr)]
    data['date'] = data['TM'].dt.date
    data['is_holiday'] = data['date'].isin(holidays).astype(int)
    data['is_day_before_holiday'] = data['date'].shift(-1).isin(holidays).astype(int)
    data['is_day_after_holiday']  = data['date'].shift(1).isin(holidays).astype(int)
    data.drop(columns=['date'], inplace=True)

    # 9) ì£¼ë§ í”Œë˜ê·¸
    data['is_weekend'] = data['weekday'].isin([5,6]).astype(int)

    # 10) ì‹œê°„ëŒ€ ë²”ì£¼í™”
    data['time_period'] = np.where(data['hour'].isin([7,8,9]), 'ì¶œê·¼',
                             np.where(data['hour'].isin([17,18,19]), 'í‡´ê·¼',
                             np.where((data['hour']>9)&(data['hour']<17), 'ë‚®',
                             np.where((data['hour']>19)&(data['hour']<21), 'ì €ë…',
                             'ë°¤'))))

    # ìˆœì„œí˜• ë²”ì£¼ ì¸ì½”ë”©
    direction_order   = ['ìƒì„ ','í•˜ì„ ','ì™¸ì„ ','ë‚´ì„ ']
    time_period_order = ['ë°¤','ì¶œê·¼','ë‚®','ì €ë…','í‡´ê·¼']
    data['Direction']   = data['Direction'].astype(
        CategoricalDtype(categories=direction_order, ordered=True)
    ).cat.codes
    data['time_period'] = data['time_period'].astype(
        CategoricalDtype(categories=time_period_order, ordered=True)
    ).cat.codes

    # 11) ì£¼ê¸°ì„± ë³€ìˆ˜ (sin/cos)
    data['sin_hod'] = np.sin(data['hour'] * (2*np.pi/24))
    data['cos_hod'] = np.cos(data['hour'] * (2*np.pi/24))
    data['sin_dow'] = np.sin(data['weekday'] * (2*np.pi/7))
    data['cos_dow'] = np.cos(data['weekday'] * (2*np.pi/7))
    # data['sin_dom'] = np.sin(data['day'] * (2*np.pi/31))
    # data['cos_dom'] = np.cos(data['day'] * (2*np.pi/31))
    # data['sin_wom'] = np.sin(data['week_of_month'] * (2*np.pi/5))
    # data['cos_wom'] = np.cos(data['week_of_month'] * (2*np.pi/5))
    # data['sin_woy'] = np.sin(data['week_of_year'] * (2*np.pi/52))
    # data['cos_woy'] = np.cos(data['week_of_year'] * (2*np.pi/52))
    data['sin_doy'] = np.sin(data['day_of_year'] * (2*np.pi/365))
    data['cos_doy'] = np.cos(data['day_of_year'] * (2*np.pi/365))


    # 12) ì„ í˜• ë³´ê°„
    cols_to_fill = ['WD','RN_DAY','RN_HR1','TA','ta_chi','SI','HM','WS']
    data[cols_to_fill] = data[cols_to_fill].interpolate(method='linear', limit_direction='both')

    print('ë³´ê°„ í›„ ë‚¨ì€ ê²°ì¸¡ê°’:\n', data[cols_to_fill].isna().sum())
    return data

# ì‚¬ìš© ì˜ˆì‹œ
df_processed  = preprocessing(df,  t, address)
test_processed = preprocessing(test, t, address)
print('ì™„ë£Œ')

# â–¶ ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡
ordered_cols = ['Direction', 'time_period']
cat_cols = ['Line', 'address', 'station_name']
num_cols = [
    'HM', 'RN_DAY', 'RN_HR1', 'SI', 'STN', 'TA', 'WD', 'WS',
    'cos_dom', 'cos_dow', 'cos_doy', 'cos_hod', 'cos_wom', 'cos_woy', 'day', 'day_of_year',
    'hour', 'is_day_after_holiday', 'is_day_before_holiday', 'is_holiday', 'is_weekend',
    'month', 'sin_dom', 'sin_dow', 'sin_doy', 'sin_hod', 'sin_wom', 'sin_woy',
    'ta_chi', 'transfer', 'week_of_month', 'week_of_year', 'weekday', 'year','station_number'
]
cat_cols = cat_cols + ordered_cols
features = num_cols + ordered_cols + cat_cols + ['Congestion']

# â–¶ ì—°ë„ë³„ ë¶„ë¦¬
# train_df = pd.concat([df[df['year'] == 0], df[df['year'] == 1]])[features]
# val_df = df[df['year'] == 2][features]

# # â–¶ category ì¸ì½”ë”©
# for col in ['Line', 'station_name', 'address']:
#     train_df[col] = train_df[col].astype('category')
#     val_df[col] = val_df[col].astype('category')
#     val_df[col] = val_df[col].cat.set_categories(train_df[col].cat.categories)
#     train_df[col] = train_df[col].cat.codes
#     val_df[col] = val_df[col].cat.codes

# X_train = train_df.drop(columns=['Congestion','STN'])
# X_val = val_df.drop(columns=['Congestion','STN'])

# y_train = train_df['Congestion'].values
# y_val = val_df['Congestion'].values

# # â–¶ ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡
# ordered_cols = ['Direction', 'time_period']
# cat_cols = ['Line', 'address', 'station_name']
# num_cols = [
#     'HM', 'RN_DAY', 'RN_HR1', 'SI', 'STN', 'TA', 'WD', 'WS', 'Station_number',
#     'cos_dom', 'cos_dow', 'cos_doy', 'cos_hod', 'cos_wom', 'cos_woy', 'day', 'day_of_year',
#     'hour', 'is_day_after_holiday', 'is_day_before_holiday', 'is_holiday', 'is_weekend',
#     'month', 'sin_dom', 'sin_dow', 'sin_doy', 'sin_hod', 'sin_wom', 'sin_woy',
#     'ta_chi', 'transfer', 'week_of_month', 'week_of_year', 'weekday', 'year'
# ]

# features = num_cols + ordered_cols + cat_cols + ['Congestion']
# df = df[features]

###### ì „ì²˜ë¦¬
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score

# ëª¨ë¸
import xgboost as xgb
import joblib

# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
results = []
final_results = []

# ê²½ê³  ì œê±° (ì„ íƒ)
import warnings
warnings.filterwarnings('ignore')

# feature & target ì¶”ì¶œ
feature_cols = features
target_col = 'Congestion'

# ğŸš‡ Lineë³„ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
for line in sorted(df_processed['Line'].unique()):
    print(f"\nğŸ“˜ [Line {line}] ëª¨ë¸ í•™ìŠµ ì‹œì‘")
   # if line!=8: continue
    df_line = df_processed[df_processed['Line'] == line].copy()
    test_line = test_processed[test_processed['Line'] == line].copy()

    # ë²”ì£¼í˜• ì²˜ë¦¬
    for col in cat_cols:
        df_line[col] = df_line[col].astype('category')
        test_line[col] = test_line[col].astype('category')

    # âœ… hour ê¸°ì¤€ ì •ë ¬ (í•µì‹¬)
    df_line = df_line.sort_values('TM')

    
    X = df_line[feature_cols]
    y = df_line[target_col].astype(int)
    X_test = test_line[feature_cols]

    # ì¸ì½”ë”©
    if line ==8:
        # 1ï¸âƒ£ ì›-í•« ì¸ì½”ë”©
        X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)
        X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)
        
        # 2ï¸âƒ£ ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (ë‘˜ ë‹¤ì—ì„œ í™•ì‹¤íˆ ì œê±°)
        X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]
        X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]
        
        # 3ï¸âƒ£ í…ŒìŠ¤íŠ¸ì…‹ ì»¬ëŸ¼ì„ í•™ìŠµì…‹ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)


    else:
        X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)
        X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)
        X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)

    # ì •ê·œí™”
    mm = MinMaxScaler()
    X_scaled = mm.fit_transform(X_enc)
    X_test_scaled = mm.transform(X_test_enc)

    # âœ… hour ìˆœ ë¶„í•  (train:val = 8:2)
    split_idx = int(len(X_scaled) * 0.8)
    X_train = X_scaled[:split_idx]
    X_val   = X_scaled[split_idx:]
    y_train = y.values[:split_idx]
    y_val   = y.values[split_idx:]

    # ëª¨ë¸ ì •ì˜
    model = xgb.XGBRegressor(
        n_estimators=1500,
        learning_rate=0.01,
        max_depth=12,
        subsample=0.9,
        colsample_bytree=0.9,
        min_child_weight=1,
        gamma=0.5,
        reg_alpha=0.3,
        reg_lambda=0.8,
        tree_method='hist',
        early_stopping_rounds=100,
        random_state=42
    )

    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)

    # ì„±ëŠ¥ í‰ê°€
    val_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    r2 = r2_score(y_val, val_pred)
    print(f"âœ… RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")
    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})

    # ì˜ˆì¸¡
    y_pred = np.round(model.predict(X_test_scaled)).astype(int)
    y_pred = np.where(y_pred < 0, 0, y_pred)
    temp = test_line[['hour', 'Line', 'station_number']].copy()
    temp['ì˜ˆì¸¡í˜¼ì¡ë„'] = y_pred
    final_results.append(temp)

    # ëª¨ë¸ ì €ì¥
    save_dir= 'models'
    model_path = f"./{save_dir}/xgb_line{line}.pkl"
    joblib.dump(model, model_path)
test_processed
# ğŸ” ê²°ê³¼ í†µí•© ë° ì €ì¥
final_df = pd.concat(final_results)
output_df = final_df[['ì˜ˆì¸¡í˜¼ì¡ë„']].rename(columns={'ì˜ˆì¸¡í˜¼ì¡ë„': 'Congestion'})

output_df.to_csv(
    './test/best_model_result.csv',
    index=False,
    encoding='utf-8'
)

# ğŸ“Š ì„±ëŠ¥ ìš”ì•½
results_df = pd.DataFrame(results)
print(results_df)

gap = pd.read_csv('./test/best_model_result.csv')
gap.shape

import os
ì œì¶œ = pd.read_csv('./test/250206.csv')

import numpy as np

# ë§ˆìŠ¤í¬ ìƒì„±
from sklearn.metrics import root_mean_squared_error
rmse = mean_squared_error(
    gap['Congestion'],
    sub['Congestion'],
    squared=False      # squared=False í•˜ë©´ RMSE ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¤Œ
)

print(f"RMSE: {rmse:.4f}")

ì œì¶œ.describe().round()

