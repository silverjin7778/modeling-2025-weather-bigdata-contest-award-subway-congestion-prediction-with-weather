{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f967d4-3e19-4870-84c0-667d20025d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 데이터_전처리_파이프라인.ipynb to script\n",
      "[NbConvertApp] Writing 27371 bytes to 데이터_전처리_파이프라인.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 데이터_전처리_파이프라인.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88ddbe-0e1e-4f2d-94e1-087245ae94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# 데이터 불러오기\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 불러오기\n",
    "test = pd.read_csv('./test/test.csv', encoding='CP949')\n",
    "df23 = pd.read_csv('./data/train_subway23.csv', encoding='CP949')\n",
    "df22 = pd.read_csv('./data/train_subway22.csv', encoding='CP949')\n",
    "df21 = pd.read_csv('./data/train_subway21.csv', encoding='CP949')\n",
    "\n",
    "# 2021, 2022, ,2023년 데이터 하나의 데이터프레임으로 병합\n",
    "df = pd.concat([df21, df22, df23], axis=0, ignore_index=True)\n",
    "\n",
    "# 환승역 데이터 불러오기\n",
    "t = pd.read_excel('./data/환승역.xlsx', names =['Line','station_name','transfer'], header=0)\n",
    "\n",
    "# 지하철역 주소 데이터 불러오기\n",
    "address = pd.read_csv('./data/result_address.csv', encoding='CP949')\n",
    "\n",
    "# address 데이터에 스크래핑에서 빠진 주소 추가해서 병합\n",
    "subway_13 = pd.DataFrame({'역명':['성수E', '응암S','불암산']\n",
    "             ,'주소':['서울 성동구 아차산로 100','서울 은평구 증산로 477','서울 노원구 상계로 305']})\n",
    "\n",
    "address = pd.concat([address, subway_13], axis=0).reset_index(drop=True)\n",
    "\n",
    "df.shape\n",
    "\n",
    "- address 정합성을 위해 전처리\n",
    "\n",
    "1. 역 이름에서 괄호 제거해서 이름 통일  \n",
    "역 이름에서 괄호 및 괄호 안 내용 제거  \n",
    "예: '강남(2호선)' → '강남'\n",
    "\n",
    "2. 지하철역 주소는 ㅇㅇ구, 인천, 경기로 통일  \n",
    "'서울특별시 강남구 ...' → '강남구'  \n",
    "'경기도 수원시 ...' → '경기도'  \n",
    "  \n",
    "3. 인천, 경기 주소 처리  \n",
    "인천, 경기에 속하는 지하철역은 인천, 경기로 주소값 통일  \n",
    "그 외는 원래 값 유지\n",
    "\n",
    "address.columns=['station_name','address']\n",
    "address.station_name = address.station_name.apply(lambda x: x.split('(')[0].strip() if '(' in x else x)\n",
    "address.address = address.address.apply(lambda x: x.split()[0] if '서울' not in x else x.split()[1])\n",
    "addr = address['address']  \n",
    "address['address'] = np.where(addr.str.contains('인천'), '인천',np.where(addr.str.contains('경기'), '경기', addr))\n",
    "\n",
    "# 전처리 파이프라인\n",
    "- **대회 공지사항에 따라 df 이상치 드롭**\n",
    "    - <공지사항>\n",
    "    - 남위례를 제외한 한대앞~오이도역 구간은 내부 프로그램 오류로 인하여 22년 6월 13일까지 4호선 재차인원이 누락되었습니다. \n",
    "    - 해당기간동안 한대앞~오이도역을 이용하는 인원은 모두 수인분당선을 이용하는것으로 기록되었습니다.\n",
    "    - 남위례역은 21년 12월 18일에 개통하였으며, 프로그램 내부에 개통사항 반영이 늦어져 혼잡도가 0으로 산출된 것으로 확인됩니다.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from holidayskr import year_holidays\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocessing(data, t, address, is_train=True, known_stations=None):\n",
    "    data = data.copy()\n",
    "    # 1)  → datetime\n",
    "    data['TM'] = data['TM'].astype(str)\n",
    "    data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H')\n",
    "\n",
    "    # 2) 범주형 변환\n",
    "    cat_columns = ['Line', 'station_number', 'STN', 'station_name', 'Direction']\n",
    "    for col in cat_columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "    # 3) 결측값 placeholder\n",
    "    data['WD']     = data['WD'].where(data['WD'] >=   0, np.nan)\n",
    "    data['WS']     = data['WS'].replace(-99.0,          np.nan)\n",
    "    data['RN_DAY'] = data['RN_DAY'].replace(-99.0,      np.nan)\n",
    "    data['RN_HR1'] = data['RN_HR1'].replace(-99.0,      np.nan)\n",
    "    data['TA']     = data['TA'].replace(-99.0,          np.nan)\n",
    "    data['ta_chi'] = data['ta_chi'].replace(-99.0,      np.nan)\n",
    "    data['SI']     = data['SI'].replace(-99.0,          np.nan)\n",
    "    data['HM']     = data['HM'].replace(-99.0,          np.nan)\n",
    "    data = data.drop(columns='SI', axis=1)\n",
    "    \n",
    "    # 5) station_name 교정\n",
    "    data['station_name'] = data['station_name'].astype(str).replace({\n",
    "        '당고개': '불암산',\n",
    "        '자양(뚝섬한강공원)': '자양',\n",
    "        '신촌(지하)': '신촌'\n",
    "    })\n",
    "    # 6) 신설역 변수 생성\n",
    "    new_station_list = {'구리', '다산', '동구릉', '별내', '암사역사공원', '장자호수공원'}\n",
    "    known_stations = data['STN'].unique()\n",
    "    \n",
    "    # 7) 신규관측소 변수\n",
    "    if is_train:\n",
    "        # 학습 데이터는 기준이 될 known_stations를 만들어 두고\n",
    "        data['신설역'] = 0\n",
    "        data['신규관측소'] = 0\n",
    "        이상치_4호선_역명 = ['한대앞','중앙','고잔','초지','안산','신길온천','정왕','오이도','개봉'] # 개봉은 그냥 여기 추가해서 처리함\n",
    "        이상치_8호선_역명 = ['남위례']\n",
    "        \n",
    "        pattern_8 = '|'.join(이상치_8호선_역명)\n",
    "        pattern_4 = '|'.join(이상치_4호선_역명)\n",
    "        \n",
    "        mask_8 = (data['Line'] == 8) & data['station_name'].str.contains(pattern_8) & (data['TM'] < '2022-12-31')\n",
    "        mask_4 = (data['Line'] == 4) & data['station_name'].str.contains(pattern_4) & (data['TM'] <= '2022-12-31')\n",
    "        print('train셋 공지사항 이상치 ',(data.shape[0] - data[~(mask_8 | mask_4)].reset_index(drop=True).shape[0])/data.shape[0],'% 제거')\n",
    "        data = data[~(mask_8 | mask_4)].reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        # test에서는 train에서 넘어온 known_stations를 이용해 플래그 처리\n",
    "        data['신설역'] = data['station_name'].apply(lambda x: 0 if x in known_stations else 1)\n",
    "        data['신규관측소'] = data['STN'].apply(lambda x: 0 if x in known_stations else 1)\n",
    "\n",
    "    # 7 ) 선형 보간\n",
    "    cols_to_interp = ['TA', 'WD', 'WS', 'RN_DAY', 'RN_HR1', 'ta_chi','HM']\n",
    "    data[cols_to_interp] = data[cols_to_interp].interpolate(method='linear', limit_direction='both')\n",
    "    data[cols_to_interp] = data[cols_to_interp].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # 8) 외부 테이블 병합\n",
    "    data = data.merge(t, on=['Line','station_name'], how='left')\n",
    "    data['transfer'] = data['transfer'].fillna(0).astype(int)\n",
    "    data = data.merge(address, on=['station_name'], how='left')\n",
    "\n",
    "    # 9) 파생 변수\n",
    "    data['year']         = data['TM'].dt.year - 2021\n",
    "    data['month']        = data['TM'].dt.month\n",
    "    data['day']          = data['TM'].dt.day\n",
    "    data['hour']         = data['TM'].dt.hour\n",
    "    data['weekday']      = data['TM'].dt.dayofweek\n",
    "    data['week_of_month']= (data['day'] - 1) // 7 + 1\n",
    "    data['week_of_year'] = data['TM'].dt.isocalendar().week.astype(int)\n",
    "    data['day_of_year']  = data['TM'].dt.dayofyear\n",
    "\n",
    "    # 10) 공휴일 플래그\n",
    "    holidays = []\n",
    "    for yr in [2021,2022,2023,2024]:\n",
    "        holidays += [d for d,_ in year_holidays(yr)]\n",
    "    data['is_holiday']            = data['TM'].dt.date.isin(holidays).astype(int)\n",
    "    data['is_day_before_holiday'] = data['TM'].dt.date.shift(-1).isin(holidays).astype(int)\n",
    "    data['is_day_after_holiday']  = data['TM'].dt.date.shift(1).isin(holidays).astype(int)\n",
    "\n",
    "    # 11) 주말 플래그\n",
    "    data['is_weekend'] = data['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "    # 12) 시간대 범주\n",
    "    data['time_period'] = np.where(data['hour'].isin([7,8,9]), '출근',\n",
    "                             np.where(data['hour'].isin([17,18,19]), '퇴근',\n",
    "                             np.where((data['hour']>9)&(data['hour']<17), '낮',\n",
    "                             np.where((data['hour']>19)&(data['hour']<21), '저녁',\n",
    "                             '밤'))))\n",
    "    direction_order   = ['상선','하선','외선','내선']\n",
    "    time_period_order = ['밤','출근','낮','저녁','퇴근']\n",
    "    data['Direction']   = data['Direction'].astype(\n",
    "        CategoricalDtype(categories=direction_order, ordered=True)\n",
    "    ).cat.codes\n",
    "    data['time_period'] = data['time_period'].astype(\n",
    "        CategoricalDtype(categories=time_period_order, ordered=True)\n",
    "    ).cat.codes\n",
    "\n",
    "    # 13) 주기성 sin/cos (24h, 7d, 31d, 5w, 52w, 365d)\n",
    "    data['sin_hod'] = np.sin(2*np.pi * data['hour']        / 24)\n",
    "    data['cos_hod'] = np.cos(2*np.pi * data['hour']        / 24)\n",
    "    data['sin_dow'] = np.sin(2*np.pi * data['weekday']     / 7)\n",
    "    data['cos_dow'] = np.cos(2*np.pi * data['weekday']     / 7)\n",
    "    data['sin_dom'] = np.sin(2*np.pi * data['day']         / 31)\n",
    "    data['cos_dom'] = np.cos(2*np.pi * data['day']         / 31)\n",
    "    data['sin_wom'] = np.sin(2*np.pi * data['week_of_month'] / 5)\n",
    "    data['cos_wom'] = np.cos(2*np.pi * data['week_of_month'] / 5)\n",
    "    data['sin_woy'] = np.sin(2*np.pi * data['week_of_year']  / 52)\n",
    "    data['cos_woy'] = np.cos(2*np.pi * data['week_of_year']  / 52)\n",
    "    data['sin_doy'] = np.sin(2*np.pi * data['day_of_year']   / 365)\n",
    "    data['cos_doy'] = np.cos(2*np.pi * data['day_of_year']   / 365)\n",
    "\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 전처리\n",
    "df   = preprocessing(df,   t, address, is_train=True)\n",
    "\n",
    "test = preprocessing(test, t, address,\n",
    "                               is_train=False,\n",
    "                               known_stations=known_stations)\n",
    "\n",
    "print('전처리 완료')\n",
    "\n",
    "df.to_parquet('data.parquet')\n",
    "test.to_parquet('test.parquet')\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 모델 저장 디렉토리\n",
    "results = []\n",
    "final_results = []\n",
    "\n",
    "model_dir = './models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for line in sorted(df['Line'].unique()):\n",
    "    #if line!=8: continue\n",
    "    print(f\"\\n📘 [Line {line}] 모델 학습 시작\")\n",
    "\n",
    "    df_line = df[df['Line'] == line].copy()\n",
    "    test_line = test[test['Line'] == line].copy()\n",
    "\n",
    "    # 범주형 처리\n",
    "    for col in ['Line', 'STN','address']:\n",
    "        df_line[col] = df_line[col].astype('category')\n",
    "        test_line[col] = test_line[col].astype('category')\n",
    "\n",
    "    # ✅ 시간 기준 정렬 (핵심)\n",
    "    df_line = df_line.sort_values('TM')\n",
    "\n",
    "    # feature & target 추출\n",
    "    X = df_line[feature_cols]\n",
    "    y = df_line[target_col].astype(int)\n",
    "    X_test = test_line[feature_cols]\n",
    "\n",
    "    # 인코딩\n",
    "    # 원-핫 인코딩\n",
    "    X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=False, prefix=cat_cols)\n",
    "    X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=False, prefix=cat_cols)\n",
    "\n",
    "    # ✅ 컬럼 정렬 및 누락된 컬럼 채움\n",
    "    X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "    \n",
    "    # 정규화\n",
    "    mm = MinMaxScaler()\n",
    "    X_scaled = mm.fit_transform(X_enc)\n",
    "    X_test_scaled = mm.transform(X_test_enc)\n",
    "\n",
    "    # ✅ 시간 순 분할 (train:val = 8:2)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train = X_scaled[:split_idx]\n",
    "    X_val   = X_scaled[split_idx:]\n",
    "    y_train = y.values[:split_idx]\n",
    "    y_val   = y.values[split_idx:]\n",
    "\n",
    "    # 모델 정의\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.8,\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "    # 성능 평가\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    print(f\"✅ RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "    # 예측\n",
    "    y_pred = np.round(model.predict(X_test_scaled)).astype(int)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    temp = test_line[['hour', 'Line', 'station_number']].copy()\n",
    "    temp['예측혼잡도'] = y_pred\n",
    "    final_results.append(temp)\n",
    "\n",
    "    # 모델 저장\n",
    "    model_path = f\"./models/xgb_line{line}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "# 🔁 결과 통합 및 저장\n",
    "final_df = pd.concat(final_results)\n",
    "output_df = final_df[['예측혼잡도']].rename(columns={'예측혼잡도': 'Congestion'})\n",
    "\n",
    "output_df.to_csv(\n",
    "    './test/250206-2.csv',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# 📊 성능 요약\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "gap = pd.read_csv('./test/250206-2.csv') # 내 데터터\n",
    "gap.shape\n",
    "\n",
    "import os\n",
    "제출 = pd.read_csv('./test/minjeong.csv') # 민정언니 데이터\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 마스크 생성\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = mean_squared_error(\n",
    "    제출['Congestion'],\n",
    "    gap['Congestion'],\n",
    "    squared=False      # squared=False 하면 RMSE 를 직접 계산해 줌\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 모델 저장 디렉토리\n",
    "results = []\n",
    "final_results = []\n",
    "\n",
    "model_dir = './models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for line in sorted(df['Line'].unique()):\n",
    "    #if line!=8: continue\n",
    "    print(f\"\\n📘 [Line {line}] 모델 학습 시작\")\n",
    "\n",
    "    df_line = df[df['Line'] == line].copy()\n",
    "    test_line = test[test['Line'] == line].copy()\n",
    "\n",
    "    # 범주형 처리\n",
    "    for col in ['Line', 'STN','address']:\n",
    "        df_line[col] = df_line[col].astype('category')\n",
    "        test_line[col] = test_line[col].astype('category')\n",
    "\n",
    "    # ✅ 시간 기준 정렬 (핵심)\n",
    "    df_line = df_line.sort_values('TM')\n",
    "\n",
    "    # feature & target 추출\n",
    "    X = df_line[feature_cols]\n",
    "    y = df_line[target_col].astype(int)\n",
    "    X_test = test_line[feature_cols]\n",
    "\n",
    "    # 인코딩\n",
    "    # 원-핫 인코딩\n",
    "    X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=False)\n",
    "    X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    # ✅ 컬럼 정렬 및 누락된 컬럼 채움\n",
    "    X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "    \n",
    "    # 정규화\n",
    "    mm = MinMaxScaler()\n",
    "    X_scaled = mm.fit_transform(X_enc)\n",
    "    X_test_scaled = mm.transform(X_test_enc)\n",
    "\n",
    "    # ✅ 시간 순 분할 (train:val = 8:2)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train = X_scaled[:split_idx]\n",
    "    X_val   = X_scaled[split_idx:]\n",
    "    y_train = y.values[:split_idx]\n",
    "    y_val   = y.values[split_idx:]\n",
    "\n",
    "    # 모델 정의\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.8,\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "    # 성능 평가\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    print(f\"✅ RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "    # 예측\n",
    "    y_pred = np.round(model.predict(X_test_scaled)).astype(int)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    temp = test_line[['hour', 'Line', 'station_number']].copy()\n",
    "    temp['예측혼잡도'] = y_pred\n",
    "    final_results.append(temp)\n",
    "\n",
    "    # 모델 저장\n",
    "    model_path = f\"./models/xgb_line{line}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "# 🔁 결과 통합 및 저장\n",
    "final_df = pd.concat(final_results)\n",
    "output_df = final_df[['예측혼잡도']].rename(columns={'예측혼잡도': 'Congestion'})\n",
    "\n",
    "output_df.to_csv(\n",
    "    './test/250206-2.csv',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# 📊 성능 요약\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "![image.png](attachment:c00437b1-3e0f-4751-a41c-3aa9ac86f320.png)![image.png](attachment:4064ece4-c190-41d0-a10c-6170728e926e.png)\n",
    "\n",
    "gap = pd.read_csv('./test/250206.csv') # 내 데터터\n",
    "gap.shape\n",
    "\n",
    "import os\n",
    "제출 = pd.read_csv('./test/250206-1.csv') # 민정언니 데이터\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 마스크 생성\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = mean_squared_error(\n",
    "    제출['Congestion'],\n",
    "    gap['Congestion'],\n",
    "    squared=False      # squared=False 하면 RMSE 를 직접 계산해 줌\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# # 2) 프로파일 리포트 생성\n",
    "# from pycaret.regression import *\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "# profile = ProfileReport(\n",
    "#     df,\n",
    "#     title=\"My Data Profiling Report\",  # 리포트 제목\n",
    "#     explorative=True,                  # 자세한 분석 모드\n",
    "#     minimal=False                       # 최소 리포트 모드 해제\n",
    "# )\n",
    "\n",
    "# # 3) 결과를 HTML 파일로 저장\n",
    "# profile.to_file(\"data_report.html\")\n",
    "\n",
    "from pycaret.regression import (\n",
    "    setup, compare_models, tune_model,\n",
    "    finalize_model, predict_model, save_model\n",
    ")\n",
    "\n",
    "os.makedirs('./models_pycaret', exist_ok=True)\n",
    "final_preds = []\n",
    "\n",
    "for line in sorted(df['Line'].unique()):\n",
    "    if line!=7: continue \n",
    "    print(f\"\\n▶▶ Line {line} AutoML 시작\")\n",
    "\n",
    "    # 4-1) Line subset\n",
    "    train_line = df[df['Line']==line].copy()\n",
    "    test_line  = test[test['Line']==line].copy()\n",
    "    train_line['Congestion'] = train_line['Congestion'].astype(int)\n",
    "    # 4-2) PyCaret setup\n",
    "    exp = setup(\n",
    "        data=train_line,\n",
    "        target='Congestion',\n",
    "        session_id=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 4-3) 모델 비교 & 선택\n",
    "    best      = compare_models(verbose=1)           # 기본 교차검증으로 최적 모델 선택\n",
    "    compare_results = pull()             # compare_models 결과 DataFrame\n",
    "    print(compare_results)\n",
    "    save_model(best, f'./models_pycaret/pycaret_base_line{line}')\n",
    "    \n",
    "    tuned     = tune_model(best, verbose=1)           # 선택된 모델 하이퍼튜닝\n",
    "    tune_results = pull()                # tune_model 결과 DataFrame\n",
    "    print(tune_results)\n",
    "    final_mod = finalize_model(tuned)      # 튜닝된 모델 파이널라이즈\n",
    "    \n",
    "    # 4-4) 테스트셋 예측\n",
    "    preds = predict_model(final_mod, data=test_line)\n",
    "    print(pull())\n",
    "    # PyCaret 회귀의 경우 예측 결과는 컬럼명 'Label'에 담겨 있습니다.\n",
    "    preds = preds.rename(columns={'Label':'Congestion_pred'}) \\\n",
    "                 [['hour','Line','station_number','Congestion_pred']]\n",
    "    preds['Congestion_pred'] = preds['Congestion_pred'].astype(int)\n",
    "    final_preds.append(preds)\n",
    "    \n",
    "    # 4-5) 모델 저장\n",
    "    save_model(final_mod, f'./models_pycaret/pycaret_tuned_line{line}')\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5) 결과 통합 및 저장\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "submission = pd.concat(final_preds).reset_index(drop=True)\n",
    "submission = submission[['hour','Line','station_number','Congestion_pred']] \\\n",
    "             .rename(columns={'Congestion_pred':'Congestion'})\n",
    "\n",
    "submission.to_csv('./test/250206_pycaret_submission.csv',\n",
    "                  index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n✅ AutoML 완료, 제출 파일 생성: ./test/250206_pycaret_submission.csv\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from holidayskr import year_holidays\n",
    "\n",
    "def preprocessing(data, t, address):\n",
    "    # 1) TM → datetime, 정렬\n",
    "    data['TM'] = data['TM'].astype(str)\n",
    "    data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H')\n",
    "\n",
    "    # 2) 범주형 변환\n",
    "    cat_columns = ['Line', 'station_number', 'STN', 'station_name', 'Direction']\n",
    "    for col in cat_columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "    # 3) 결측값 처리용 placeholder\n",
    "    data['WD'] = data['WD'].where(data['WD'] >= 0, np.nan)\n",
    "    data['WS'] = data['WS'].replace(-99.0, np.nan)\n",
    "    data['RN_DAY'] = data['RN_DAY'].replace(-99.0, np.nan)\n",
    "    data['RN_HR1'] = data['RN_HR1'].replace(-99.0, np.nan)\n",
    "    data['TA'] = data['TA'].replace(-99.0, np.nan)\n",
    "    data['ta_chi'] = data['ta_chi'].replace(-99.0, np.nan)\n",
    "    data['SI'] = data['SI'].replace(-99.0, np.nan)\n",
    "    data['HM'] = data['HM'].replace(-99.0, np.nan)\n",
    "\n",
    "    # 4) 이진 플래그화\n",
    "    data['SI'] = data['SI'].notna().astype(int)\n",
    "\n",
    "    # 5) station_name 일부 교정\n",
    "    data['station_name'] = data['station_name'].astype(str).replace({\n",
    "        '당고개': '불암산',\n",
    "        '자양(뚝섬한강공원)': '자양',\n",
    "        '신촌(지하)': '신촌'\n",
    "    })\n",
    "\n",
    "    # 6) 외부 테이블 병합\n",
    "    data = pd.merge(data, t, on=['Line','station_name'], how='left')\n",
    "    data['transfer'] = data['transfer'].fillna(0).astype(int)\n",
    "\n",
    "    data = pd.merge(data, address, on=['station_name'], how='left')\n",
    "\n",
    "    # 7) 키, 시간 파생\n",
    "    data['key']   = data['Line'].astype(str) + '_' + data['station_name'].astype(str) + '_' + data['Direction'].astype(str)\n",
    "    data['year']  = data['TM'].dt.year - 2021\n",
    "    data['month'] = data['TM'].dt.month\n",
    "    data['day']   = data['TM'].dt.day\n",
    "    data['hour']  = data['TM'].dt.hour\n",
    "    data['weekday']      = data['TM'].dt.weekday\n",
    "    data['week_of_month']= (data['TM'].dt.day.sub(1) // 7) + 1\n",
    "    data['week_of_year'] = data['TM'].dt.isocalendar().week.astype(int)\n",
    "    data['day_of_year']  = data['TM'].dt.dayofyear\n",
    "\n",
    "    # 8) 공휴일 플래그\n",
    "    holidays = []\n",
    "    for yr in [2021, 2022, 2023, 2024]:\n",
    "        holidays += [d for d, _ in year_holidays(yr)]\n",
    "    data['date'] = data['TM'].dt.date\n",
    "    data['is_holiday'] = data['date'].isin(holidays).astype(int)\n",
    "    data['is_day_before_holiday'] = data['date'].shift(-1).isin(holidays).astype(int)\n",
    "    data['is_day_after_holiday']  = data['date'].shift(1).isin(holidays).astype(int)\n",
    "    data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    # 9) 주말 플래그\n",
    "    data['is_weekend'] = data['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "    # 10) 시간대 범주화\n",
    "    data['time_period'] = np.where(data['hour'].isin([7,8,9]), '출근',\n",
    "                             np.where(data['hour'].isin([17,18,19]), '퇴근',\n",
    "                             np.where((data['hour']>9)&(data['hour']<17), '낮',\n",
    "                             np.where((data['hour']>19)&(data['hour']<21), '저녁',\n",
    "                             '밤'))))\n",
    "\n",
    "    # 순서형 범주 인코딩\n",
    "    direction_order   = ['상선','하선','외선','내선']\n",
    "    time_period_order = ['밤','출근','낮','저녁','퇴근']\n",
    "    data['Direction']   = data['Direction'].astype(\n",
    "        CategoricalDtype(categories=direction_order, ordered=True)\n",
    "    ).cat.codes\n",
    "    data['time_period'] = data['time_period'].astype(\n",
    "        CategoricalDtype(categories=time_period_order, ordered=True)\n",
    "    ).cat.codes\n",
    "\n",
    "    # 11) 주기성 변수 (sin/cos)\n",
    "    data['sin_hod'] = np.sin(data['hour'] * (2*np.pi/24))\n",
    "    data['cos_hod'] = np.cos(data['hour'] * (2*np.pi/24))\n",
    "    data['sin_dow'] = np.sin(data['weekday'] * (2*np.pi/7))\n",
    "    data['cos_dow'] = np.cos(data['weekday'] * (2*np.pi/7))\n",
    "    # data['sin_dom'] = np.sin(data['day'] * (2*np.pi/31))\n",
    "    # data['cos_dom'] = np.cos(data['day'] * (2*np.pi/31))\n",
    "    # data['sin_wom'] = np.sin(data['week_of_month'] * (2*np.pi/5))\n",
    "    # data['cos_wom'] = np.cos(data['week_of_month'] * (2*np.pi/5))\n",
    "    # data['sin_woy'] = np.sin(data['week_of_year'] * (2*np.pi/52))\n",
    "    # data['cos_woy'] = np.cos(data['week_of_year'] * (2*np.pi/52))\n",
    "    data['sin_doy'] = np.sin(data['day_of_year'] * (2*np.pi/365))\n",
    "    data['cos_doy'] = np.cos(data['day_of_year'] * (2*np.pi/365))\n",
    "\n",
    "\n",
    "    # 12) 선형 보간\n",
    "    cols_to_fill = ['WD','RN_DAY','RN_HR1','TA','ta_chi','SI','HM','WS']\n",
    "    data[cols_to_fill] = data[cols_to_fill].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    print('보간 후 남은 결측값:\\n', data[cols_to_fill].isna().sum())\n",
    "    return data\n",
    "\n",
    "# 사용 예시\n",
    "df_processed  = preprocessing(df,  t, address)\n",
    "test_processed = preprocessing(test, t, address)\n",
    "print('완료')\n",
    "\n",
    "# ▶ 수치형 피처 목록\n",
    "ordered_cols = ['Direction', 'time_period']\n",
    "cat_cols = ['Line', 'address', 'station_name']\n",
    "num_cols = [\n",
    "    'HM', 'RN_DAY', 'RN_HR1', 'SI', 'STN', 'TA', 'WD', 'WS',\n",
    "    'cos_dom', 'cos_dow', 'cos_doy', 'cos_hod', 'cos_wom', 'cos_woy', 'day', 'day_of_year',\n",
    "    'hour', 'is_day_after_holiday', 'is_day_before_holiday', 'is_holiday', 'is_weekend',\n",
    "    'month', 'sin_dom', 'sin_dow', 'sin_doy', 'sin_hod', 'sin_wom', 'sin_woy',\n",
    "    'ta_chi', 'transfer', 'week_of_month', 'week_of_year', 'weekday', 'year','station_number'\n",
    "]\n",
    "cat_cols = cat_cols + ordered_cols\n",
    "features = num_cols + ordered_cols + cat_cols + ['Congestion']\n",
    "\n",
    "# ▶ 연도별 분리\n",
    "# train_df = pd.concat([df[df['year'] == 0], df[df['year'] == 1]])[features]\n",
    "# val_df = df[df['year'] == 2][features]\n",
    "\n",
    "# # ▶ category 인코딩\n",
    "# for col in ['Line', 'station_name', 'address']:\n",
    "#     train_df[col] = train_df[col].astype('category')\n",
    "#     val_df[col] = val_df[col].astype('category')\n",
    "#     val_df[col] = val_df[col].cat.set_categories(train_df[col].cat.categories)\n",
    "#     train_df[col] = train_df[col].cat.codes\n",
    "#     val_df[col] = val_df[col].cat.codes\n",
    "\n",
    "# X_train = train_df.drop(columns=['Congestion','STN'])\n",
    "# X_val = val_df.drop(columns=['Congestion','STN'])\n",
    "\n",
    "# y_train = train_df['Congestion'].values\n",
    "# y_val = val_df['Congestion'].values\n",
    "\n",
    "# # ▶ 수치형 피처 목록\n",
    "# ordered_cols = ['Direction', 'time_period']\n",
    "# cat_cols = ['Line', 'address', 'station_name']\n",
    "# num_cols = [\n",
    "#     'HM', 'RN_DAY', 'RN_HR1', 'SI', 'STN', 'TA', 'WD', 'WS', 'Station_number',\n",
    "#     'cos_dom', 'cos_dow', 'cos_doy', 'cos_hod', 'cos_wom', 'cos_woy', 'day', 'day_of_year',\n",
    "#     'hour', 'is_day_after_holiday', 'is_day_before_holiday', 'is_holiday', 'is_weekend',\n",
    "#     'month', 'sin_dom', 'sin_dow', 'sin_doy', 'sin_hod', 'sin_wom', 'sin_woy',\n",
    "#     'ta_chi', 'transfer', 'week_of_month', 'week_of_year', 'weekday', 'year'\n",
    "# ]\n",
    "\n",
    "# features = num_cols + ordered_cols + cat_cols + ['Congestion']\n",
    "# df = df[features]\n",
    "\n",
    "###### 전처리\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 모델\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "results = []\n",
    "final_results = []\n",
    "\n",
    "# 경고 제거 (선택)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# feature & target 추출\n",
    "feature_cols = features\n",
    "target_col = 'Congestion'\n",
    "\n",
    "# 🚇 Line별 모델 학습 및 예측\n",
    "for line in sorted(df_processed['Line'].unique()):\n",
    "    print(f\"\\n📘 [Line {line}] 모델 학습 시작\")\n",
    "   # if line!=8: continue\n",
    "    df_line = df_processed[df_processed['Line'] == line].copy()\n",
    "    test_line = test_processed[test_processed['Line'] == line].copy()\n",
    "\n",
    "    # 범주형 처리\n",
    "    for col in cat_cols:\n",
    "        df_line[col] = df_line[col].astype('category')\n",
    "        test_line[col] = test_line[col].astype('category')\n",
    "\n",
    "    # ✅ hour 기준 정렬 (핵심)\n",
    "    df_line = df_line.sort_values('TM')\n",
    "\n",
    "    \n",
    "    X = df_line[feature_cols]\n",
    "    y = df_line[target_col].astype(int)\n",
    "    X_test = test_line[feature_cols]\n",
    "\n",
    "    # 인코딩\n",
    "    if line ==8:\n",
    "        # 1️⃣ 원-핫 인코딩\n",
    "        X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "        X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "        \n",
    "        # 2️⃣ 중복 컬럼 제거 (둘 다에서 확실히 제거)\n",
    "        X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
    "        X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]\n",
    "        \n",
    "        # 3️⃣ 테스트셋 컬럼을 학습셋 기준으로 정렬\n",
    "        X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "        X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "        X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "\n",
    "    # 정규화\n",
    "    mm = MinMaxScaler()\n",
    "    X_scaled = mm.fit_transform(X_enc)\n",
    "    X_test_scaled = mm.transform(X_test_enc)\n",
    "\n",
    "    # ✅ hour 순 분할 (train:val = 8:2)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train = X_scaled[:split_idx]\n",
    "    X_val   = X_scaled[split_idx:]\n",
    "    y_train = y.values[:split_idx]\n",
    "    y_val   = y.values[split_idx:]\n",
    "\n",
    "    # 모델 정의\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.8,\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "    # 성능 평가\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    print(f\"✅ RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "    # 예측\n",
    "    y_pred = np.round(model.predict(X_test_scaled)).astype(int)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    temp = test_line[['hour', 'Line', 'station_number']].copy()\n",
    "    temp['예측혼잡도'] = y_pred\n",
    "    final_results.append(temp)\n",
    "\n",
    "    # 모델 저장\n",
    "    save_dir= 'models'\n",
    "    model_path = f\"./{save_dir}/xgb_line{line}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "test_processed\n",
    "# 🔁 결과 통합 및 저장\n",
    "final_df = pd.concat(final_results)\n",
    "output_df = final_df[['예측혼잡도']].rename(columns={'예측혼잡도': 'Congestion'})\n",
    "\n",
    "output_df.to_csv(\n",
    "    './test/best_model_result.csv',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# 📊 성능 요약\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "gap = pd.read_csv('./test/best_model_result.csv')\n",
    "gap.shape\n",
    "\n",
    "import os\n",
    "제출 = pd.read_csv('./test/250206.csv')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 마스크 생성\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = mean_squared_error(\n",
    "    gap['Congestion'],\n",
    "    sub['Congestion'],\n",
    "    squared=False      # squared=False 하면 RMSE 를 직접 계산해 줌\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "제출.describe().round()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
