{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f967d4-3e19-4870-84c0-667d20025d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ë°ì´í„°_ì „ì²˜ë¦¬_íŒŒì´í”„ë¼ì¸.ipynb to script\n",
      "[NbConvertApp] Writing 27371 bytes to ë°ì´í„°_ì „ì²˜ë¦¬_íŒŒì´í”„ë¼ì¸.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script ë°ì´í„°_ì „ì²˜ë¦¬_íŒŒì´í”„ë¼ì¸.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88ddbe-0e1e-4f2d-94e1-087245ae94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "test = pd.read_csv('./test/test.csv', encoding='CP949')\n",
    "df23 = pd.read_csv('./data/train_subway23.csv', encoding='CP949')\n",
    "df22 = pd.read_csv('./data/train_subway22.csv', encoding='CP949')\n",
    "df21 = pd.read_csv('./data/train_subway21.csv', encoding='CP949')\n",
    "\n",
    "# 2021, 2022, ,2023ë…„ ë°ì´í„° í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©\n",
    "df = pd.concat([df21, df22, df23], axis=0, ignore_index=True)\n",
    "\n",
    "# í™˜ìŠ¹ì—­ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "t = pd.read_excel('./data/í™˜ìŠ¹ì—­.xlsx', names =['Line','station_name','transfer'], header=0)\n",
    "\n",
    "# ì§€í•˜ì² ì—­ ì£¼ì†Œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "address = pd.read_csv('./data/result_address.csv', encoding='CP949')\n",
    "\n",
    "# address ë°ì´í„°ì— ìŠ¤í¬ë˜í•‘ì—ì„œ ë¹ ì§„ ì£¼ì†Œ ì¶”ê°€í•´ì„œ ë³‘í•©\n",
    "subway_13 = pd.DataFrame({'ì—­ëª…':['ì„±ìˆ˜E', 'ì‘ì•”S','ë¶ˆì•”ì‚°']\n",
    "             ,'ì£¼ì†Œ':['ì„œìš¸ ì„±ë™êµ¬ ì•„ì°¨ì‚°ë¡œ 100','ì„œìš¸ ì€í‰êµ¬ ì¦ì‚°ë¡œ 477','ì„œìš¸ ë…¸ì›êµ¬ ìƒê³„ë¡œ 305']})\n",
    "\n",
    "address = pd.concat([address, subway_13], axis=0).reset_index(drop=True)\n",
    "\n",
    "df.shape\n",
    "\n",
    "- address ì •í•©ì„±ì„ ìœ„í•´ ì „ì²˜ë¦¬\n",
    "\n",
    "1. ì—­ ì´ë¦„ì—ì„œ ê´„í˜¸ ì œê±°í•´ì„œ ì´ë¦„ í†µì¼  \n",
    "ì—­ ì´ë¦„ì—ì„œ ê´„í˜¸ ë° ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±°  \n",
    "ì˜ˆ: 'ê°•ë‚¨(2í˜¸ì„ )' â†’ 'ê°•ë‚¨'\n",
    "\n",
    "2. ì§€í•˜ì² ì—­ ì£¼ì†ŒëŠ” ã…‡ã…‡êµ¬, ì¸ì²œ, ê²½ê¸°ë¡œ í†µì¼  \n",
    "'ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ...' â†’ 'ê°•ë‚¨êµ¬'  \n",
    "'ê²½ê¸°ë„ ìˆ˜ì›ì‹œ ...' â†’ 'ê²½ê¸°ë„'  \n",
    "  \n",
    "3. ì¸ì²œ, ê²½ê¸° ì£¼ì†Œ ì²˜ë¦¬  \n",
    "ì¸ì²œ, ê²½ê¸°ì— ì†í•˜ëŠ” ì§€í•˜ì² ì—­ì€ ì¸ì²œ, ê²½ê¸°ë¡œ ì£¼ì†Œê°’ í†µì¼  \n",
    "ê·¸ ì™¸ëŠ” ì›ë˜ ê°’ ìœ ì§€\n",
    "\n",
    "address.columns=['station_name','address']\n",
    "address.station_name = address.station_name.apply(lambda x: x.split('(')[0].strip() if '(' in x else x)\n",
    "address.address = address.address.apply(lambda x: x.split()[0] if 'ì„œìš¸' not in x else x.split()[1])\n",
    "addr = address['address']  \n",
    "address['address'] = np.where(addr.str.contains('ì¸ì²œ'), 'ì¸ì²œ',np.where(addr.str.contains('ê²½ê¸°'), 'ê²½ê¸°', addr))\n",
    "\n",
    "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "- **ëŒ€íšŒ ê³µì§€ì‚¬í•­ì— ë”°ë¼ df ì´ìƒì¹˜ ë“œë¡­**\n",
    "    - <ê³µì§€ì‚¬í•­>\n",
    "    - ë‚¨ìœ„ë¡€ë¥¼ ì œì™¸í•œ í•œëŒ€ì•~ì˜¤ì´ë„ì—­ êµ¬ê°„ì€ ë‚´ë¶€ í”„ë¡œê·¸ë¨ ì˜¤ë¥˜ë¡œ ì¸í•˜ì—¬ 22ë…„ 6ì›” 13ì¼ê¹Œì§€ 4í˜¸ì„  ì¬ì°¨ì¸ì›ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "    - í•´ë‹¹ê¸°ê°„ë™ì•ˆ í•œëŒ€ì•~ì˜¤ì´ë„ì—­ì„ ì´ìš©í•˜ëŠ” ì¸ì›ì€ ëª¨ë‘ ìˆ˜ì¸ë¶„ë‹¹ì„ ì„ ì´ìš©í•˜ëŠ”ê²ƒìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "    - ë‚¨ìœ„ë¡€ì—­ì€ 21ë…„ 12ì›” 18ì¼ì— ê°œí†µí•˜ì˜€ìœ¼ë©°, í”„ë¡œê·¸ë¨ ë‚´ë¶€ì— ê°œí†µì‚¬í•­ ë°˜ì˜ì´ ëŠ¦ì–´ì ¸ í˜¼ì¡ë„ê°€ 0ìœ¼ë¡œ ì‚°ì¶œëœ ê²ƒìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from holidayskr import year_holidays\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocessing(data, t, address, is_train=True, known_stations=None):\n",
    "    data = data.copy()\n",
    "    # 1)  â†’ datetime\n",
    "    data['TM'] = data['TM'].astype(str)\n",
    "    data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H')\n",
    "\n",
    "    # 2) ë²”ì£¼í˜• ë³€í™˜\n",
    "    cat_columns = ['Line', 'station_number', 'STN', 'station_name', 'Direction']\n",
    "    for col in cat_columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "    # 3) ê²°ì¸¡ê°’ placeholder\n",
    "    data['WD']     = data['WD'].where(data['WD'] >=   0, np.nan)\n",
    "    data['WS']     = data['WS'].replace(-99.0,          np.nan)\n",
    "    data['RN_DAY'] = data['RN_DAY'].replace(-99.0,      np.nan)\n",
    "    data['RN_HR1'] = data['RN_HR1'].replace(-99.0,      np.nan)\n",
    "    data['TA']     = data['TA'].replace(-99.0,          np.nan)\n",
    "    data['ta_chi'] = data['ta_chi'].replace(-99.0,      np.nan)\n",
    "    data['SI']     = data['SI'].replace(-99.0,          np.nan)\n",
    "    data['HM']     = data['HM'].replace(-99.0,          np.nan)\n",
    "    data = data.drop(columns='SI', axis=1)\n",
    "    \n",
    "    # 5) station_name êµì •\n",
    "    data['station_name'] = data['station_name'].astype(str).replace({\n",
    "        'ë‹¹ê³ ê°œ': 'ë¶ˆì•”ì‚°',\n",
    "        'ìì–‘(ëšì„¬í•œê°•ê³µì›)': 'ìì–‘',\n",
    "        'ì‹ ì´Œ(ì§€í•˜)': 'ì‹ ì´Œ'\n",
    "    })\n",
    "    # 6) ì‹ ì„¤ì—­ ë³€ìˆ˜ ìƒì„±\n",
    "    new_station_list = {'êµ¬ë¦¬', 'ë‹¤ì‚°', 'ë™êµ¬ë¦‰', 'ë³„ë‚´', 'ì•”ì‚¬ì—­ì‚¬ê³µì›', 'ì¥ìí˜¸ìˆ˜ê³µì›'}\n",
    "    known_stations = data['STN'].unique()\n",
    "    \n",
    "    # 7) ì‹ ê·œê´€ì¸¡ì†Œ ë³€ìˆ˜\n",
    "    if is_train:\n",
    "        # í•™ìŠµ ë°ì´í„°ëŠ” ê¸°ì¤€ì´ ë  known_stationsë¥¼ ë§Œë“¤ì–´ ë‘ê³ \n",
    "        data['ì‹ ì„¤ì—­'] = 0\n",
    "        data['ì‹ ê·œê´€ì¸¡ì†Œ'] = 0\n",
    "        ì´ìƒì¹˜_4í˜¸ì„ _ì—­ëª… = ['í•œëŒ€ì•','ì¤‘ì•™','ê³ ì”','ì´ˆì§€','ì•ˆì‚°','ì‹ ê¸¸ì˜¨ì²œ','ì •ì™•','ì˜¤ì´ë„','ê°œë´‰'] # ê°œë´‰ì€ ê·¸ëƒ¥ ì—¬ê¸° ì¶”ê°€í•´ì„œ ì²˜ë¦¬í•¨\n",
    "        ì´ìƒì¹˜_8í˜¸ì„ _ì—­ëª… = ['ë‚¨ìœ„ë¡€']\n",
    "        \n",
    "        pattern_8 = '|'.join(ì´ìƒì¹˜_8í˜¸ì„ _ì—­ëª…)\n",
    "        pattern_4 = '|'.join(ì´ìƒì¹˜_4í˜¸ì„ _ì—­ëª…)\n",
    "        \n",
    "        mask_8 = (data['Line'] == 8) & data['station_name'].str.contains(pattern_8) & (data['TM'] < '2022-12-31')\n",
    "        mask_4 = (data['Line'] == 4) & data['station_name'].str.contains(pattern_4) & (data['TM'] <= '2022-12-31')\n",
    "        print('trainì…‹ ê³µì§€ì‚¬í•­ ì´ìƒì¹˜ ',(data.shape[0] - data[~(mask_8 | mask_4)].reset_index(drop=True).shape[0])/data.shape[0],'% ì œê±°')\n",
    "        data = data[~(mask_8 | mask_4)].reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        # testì—ì„œëŠ” trainì—ì„œ ë„˜ì–´ì˜¨ known_stationsë¥¼ ì´ìš©í•´ í”Œë˜ê·¸ ì²˜ë¦¬\n",
    "        data['ì‹ ì„¤ì—­'] = data['station_name'].apply(lambda x: 0 if x in known_stations else 1)\n",
    "        data['ì‹ ê·œê´€ì¸¡ì†Œ'] = data['STN'].apply(lambda x: 0 if x in known_stations else 1)\n",
    "\n",
    "    # 7 ) ì„ í˜• ë³´ê°„\n",
    "    cols_to_interp = ['TA', 'WD', 'WS', 'RN_DAY', 'RN_HR1', 'ta_chi','HM']\n",
    "    data[cols_to_interp] = data[cols_to_interp].interpolate(method='linear', limit_direction='both')\n",
    "    data[cols_to_interp] = data[cols_to_interp].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # 8) ì™¸ë¶€ í…Œì´ë¸” ë³‘í•©\n",
    "    data = data.merge(t, on=['Line','station_name'], how='left')\n",
    "    data['transfer'] = data['transfer'].fillna(0).astype(int)\n",
    "    data = data.merge(address, on=['station_name'], how='left')\n",
    "\n",
    "    # 9) íŒŒìƒ ë³€ìˆ˜\n",
    "    data['year']         = data['TM'].dt.year - 2021\n",
    "    data['month']        = data['TM'].dt.month\n",
    "    data['day']          = data['TM'].dt.day\n",
    "    data['hour']         = data['TM'].dt.hour\n",
    "    data['weekday']      = data['TM'].dt.dayofweek\n",
    "    data['week_of_month']= (data['day'] - 1) // 7 + 1\n",
    "    data['week_of_year'] = data['TM'].dt.isocalendar().week.astype(int)\n",
    "    data['day_of_year']  = data['TM'].dt.dayofyear\n",
    "\n",
    "    # 10) ê³µíœ´ì¼ í”Œë˜ê·¸\n",
    "    holidays = []\n",
    "    for yr in [2021,2022,2023,2024]:\n",
    "        holidays += [d for d,_ in year_holidays(yr)]\n",
    "    data['is_holiday']            = data['TM'].dt.date.isin(holidays).astype(int)\n",
    "    data['is_day_before_holiday'] = data['TM'].dt.date.shift(-1).isin(holidays).astype(int)\n",
    "    data['is_day_after_holiday']  = data['TM'].dt.date.shift(1).isin(holidays).astype(int)\n",
    "\n",
    "    # 11) ì£¼ë§ í”Œë˜ê·¸\n",
    "    data['is_weekend'] = data['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "    # 12) ì‹œê°„ëŒ€ ë²”ì£¼\n",
    "    data['time_period'] = np.where(data['hour'].isin([7,8,9]), 'ì¶œê·¼',\n",
    "                             np.where(data['hour'].isin([17,18,19]), 'í‡´ê·¼',\n",
    "                             np.where((data['hour']>9)&(data['hour']<17), 'ë‚®',\n",
    "                             np.where((data['hour']>19)&(data['hour']<21), 'ì €ë…',\n",
    "                             'ë°¤'))))\n",
    "    direction_order   = ['ìƒì„ ','í•˜ì„ ','ì™¸ì„ ','ë‚´ì„ ']\n",
    "    time_period_order = ['ë°¤','ì¶œê·¼','ë‚®','ì €ë…','í‡´ê·¼']\n",
    "    data['Direction']   = data['Direction'].astype(\n",
    "        CategoricalDtype(categories=direction_order, ordered=True)\n",
    "    ).cat.codes\n",
    "    data['time_period'] = data['time_period'].astype(\n",
    "        CategoricalDtype(categories=time_period_order, ordered=True)\n",
    "    ).cat.codes\n",
    "\n",
    "    # 13) ì£¼ê¸°ì„± sin/cos (24h, 7d, 31d, 5w, 52w, 365d)\n",
    "    data['sin_hod'] = np.sin(2*np.pi * data['hour']        / 24)\n",
    "    data['cos_hod'] = np.cos(2*np.pi * data['hour']        / 24)\n",
    "    data['sin_dow'] = np.sin(2*np.pi * data['weekday']     / 7)\n",
    "    data['cos_dow'] = np.cos(2*np.pi * data['weekday']     / 7)\n",
    "    data['sin_dom'] = np.sin(2*np.pi * data['day']         / 31)\n",
    "    data['cos_dom'] = np.cos(2*np.pi * data['day']         / 31)\n",
    "    data['sin_wom'] = np.sin(2*np.pi * data['week_of_month'] / 5)\n",
    "    data['cos_wom'] = np.cos(2*np.pi * data['week_of_month'] / 5)\n",
    "    data['sin_woy'] = np.sin(2*np.pi * data['week_of_year']  / 52)\n",
    "    data['cos_woy'] = np.cos(2*np.pi * data['week_of_year']  / 52)\n",
    "    data['sin_doy'] = np.sin(2*np.pi * data['day_of_year']   / 365)\n",
    "    data['cos_doy'] = np.cos(2*np.pi * data['day_of_year']   / 365)\n",
    "\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ì „ì²˜ë¦¬\n",
    "df   = preprocessing(df,   t, address, is_train=True)\n",
    "\n",
    "test = preprocessing(test, t, address,\n",
    "                               is_train=False,\n",
    "                               known_stations=known_stations)\n",
    "\n",
    "print('ì „ì²˜ë¦¬ ì™„ë£Œ')\n",
    "\n",
    "df.to_parquet('data.parquet')\n",
    "test.to_parquet('test.parquet')\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "results = []\n",
    "final_results = []\n",
    "\n",
    "model_dir = './models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for line in sorted(df['Line'].unique()):\n",
    "    #if line!=8: continue\n",
    "    print(f\"\\nğŸ“˜ [Line {line}] ëª¨ë¸ í•™ìŠµ ì‹œì‘\")\n",
    "\n",
    "    df_line = df[df['Line'] == line].copy()\n",
    "    test_line = test[test['Line'] == line].copy()\n",
    "\n",
    "    # ë²”ì£¼í˜• ì²˜ë¦¬\n",
    "    for col in ['Line', 'STN','address']:\n",
    "        df_line[col] = df_line[col].astype('category')\n",
    "        test_line[col] = test_line[col].astype('category')\n",
    "\n",
    "    # âœ… ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (í•µì‹¬)\n",
    "    df_line = df_line.sort_values('TM')\n",
    "\n",
    "    # feature & target ì¶”ì¶œ\n",
    "    X = df_line[feature_cols]\n",
    "    y = df_line[target_col].astype(int)\n",
    "    X_test = test_line[feature_cols]\n",
    "\n",
    "    # ì¸ì½”ë”©\n",
    "    # ì›-í•« ì¸ì½”ë”©\n",
    "    X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=False, prefix=cat_cols)\n",
    "    X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=False, prefix=cat_cols)\n",
    "\n",
    "    # âœ… ì»¬ëŸ¼ ì •ë ¬ ë° ëˆ„ë½ëœ ì»¬ëŸ¼ ì±„ì›€\n",
    "    X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    mm = MinMaxScaler()\n",
    "    X_scaled = mm.fit_transform(X_enc)\n",
    "    X_test_scaled = mm.transform(X_test_enc)\n",
    "\n",
    "    # âœ… ì‹œê°„ ìˆœ ë¶„í•  (train:val = 8:2)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train = X_scaled[:split_idx]\n",
    "    X_val   = X_scaled[split_idx:]\n",
    "    y_train = y.values[:split_idx]\n",
    "    y_val   = y.values[split_idx:]\n",
    "\n",
    "    # ëª¨ë¸ ì •ì˜\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.8,\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "    # ì„±ëŠ¥ í‰ê°€\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    print(f\"âœ… RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
    "    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = np.round(model.predict(X_test_scaled)).astype(int)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    temp = test_line[['hour', 'Line', 'station_number']].copy()\n",
    "    temp['ì˜ˆì¸¡í˜¼ì¡ë„'] = y_pred\n",
    "    final_results.append(temp)\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    model_path = f\"./models/xgb_line{line}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "# ğŸ” ê²°ê³¼ í†µí•© ë° ì €ì¥\n",
    "final_df = pd.concat(final_results)\n",
    "output_df = final_df[['ì˜ˆì¸¡í˜¼ì¡ë„']].rename(columns={'ì˜ˆì¸¡í˜¼ì¡ë„': 'Congestion'})\n",
    "\n",
    "output_df.to_csv(\n",
    "    './test/250206-2.csv',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# ğŸ“Š ì„±ëŠ¥ ìš”ì•½\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "gap = pd.read_csv('./test/250206-2.csv') # ë‚´ ë°í„°í„°\n",
    "gap.shape\n",
    "\n",
    "import os\n",
    "ì œì¶œ = pd.read_csv('./test/minjeong.csv') # ë¯¼ì •ì–¸ë‹ˆ ë°ì´í„°\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ë§ˆìŠ¤í¬ ìƒì„±\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = mean_squared_error(\n",
    "    ì œì¶œ['Congestion'],\n",
    "    gap['Congestion'],\n",
    "    squared=False      # squared=False í•˜ë©´ RMSE ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¤Œ\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "results = []\n",
    "final_results = []\n",
    "\n",
    "model_dir = './models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for line in sorted(df['Line'].unique()):\n",
    "    #if line!=8: continue\n",
    "    print(f\"\\nğŸ“˜ [Line {line}] ëª¨ë¸ í•™ìŠµ ì‹œì‘\")\n",
    "\n",
    "    df_line = df[df['Line'] == line].copy()\n",
    "    test_line = test[test['Line'] == line].copy()\n",
    "\n",
    "    # ë²”ì£¼í˜• ì²˜ë¦¬\n",
    "    for col in ['Line', 'STN','address']:\n",
    "        df_line[col] = df_line[col].astype('category')\n",
    "        test_line[col] = test_line[col].astype('category')\n",
    "\n",
    "    # âœ… ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (í•µì‹¬)\n",
    "    df_line = df_line.sort_values('TM')\n",
    "\n",
    "    # feature & target ì¶”ì¶œ\n",
    "    X = df_line[feature_cols]\n",
    "    y = df_line[target_col].astype(int)\n",
    "    X_test = test_line[feature_cols]\n",
    "\n",
    "    # ì¸ì½”ë”©\n",
    "    # ì›-í•« ì¸ì½”ë”©\n",
    "    X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=False)\n",
    "    X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    # âœ… ì»¬ëŸ¼ ì •ë ¬ ë° ëˆ„ë½ëœ ì»¬ëŸ¼ ì±„ì›€\n",
    "    X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]\n",
    "    X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    mm = MinMaxScaler()\n",
    "    X_scaled = mm.fit_transform(X_enc)\n",
    "    X_test_scaled = mm.transform(X_test_enc)\n",
    "\n",
    "    # âœ… ì‹œê°„ ìˆœ ë¶„í•  (train:val = 8:2)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train = X_scaled[:split_idx]\n",
    "    X_val   = X_scaled[split_idx:]\n",
    "    y_train = y.values[:split_idx]\n",
    "    y_val   = y.values[split_idx:]\n",
    "\n",
    "    # ëª¨ë¸ ì •ì˜\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.8,\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "    # ì„±ëŠ¥ í‰ê°€\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    print(f\"âœ… RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
    "    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = np.round(model.predict(X_test_scaled)).astype(int)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    temp = test_line[['hour', 'Line', 'station_number']].copy()\n",
    "    temp['ì˜ˆì¸¡í˜¼ì¡ë„'] = y_pred\n",
    "    final_results.append(temp)\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    model_path = f\"./models/xgb_line{line}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "# ğŸ” ê²°ê³¼ í†µí•© ë° ì €ì¥\n",
    "final_df = pd.concat(final_results)\n",
    "output_df = final_df[['ì˜ˆì¸¡í˜¼ì¡ë„']].rename(columns={'ì˜ˆì¸¡í˜¼ì¡ë„': 'Congestion'})\n",
    "\n",
    "output_df.to_csv(\n",
    "    './test/250206-2.csv',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# ğŸ“Š ì„±ëŠ¥ ìš”ì•½\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "![image.png](attachment:c00437b1-3e0f-4751-a41c-3aa9ac86f320.png)![image.png](attachment:4064ece4-c190-41d0-a10c-6170728e926e.png)\n",
    "\n",
    "gap = pd.read_csv('./test/250206.csv') # ë‚´ ë°í„°í„°\n",
    "gap.shape\n",
    "\n",
    "import os\n",
    "ì œì¶œ = pd.read_csv('./test/250206-1.csv') # ë¯¼ì •ì–¸ë‹ˆ ë°ì´í„°\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ë§ˆìŠ¤í¬ ìƒì„±\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = mean_squared_error(\n",
    "    ì œì¶œ['Congestion'],\n",
    "    gap['Congestion'],\n",
    "    squared=False      # squared=False í•˜ë©´ RMSE ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¤Œ\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# # 2) í”„ë¡œíŒŒì¼ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "# from pycaret.regression import *\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "# profile = ProfileReport(\n",
    "#     df,\n",
    "#     title=\"My Data Profiling Report\",  # ë¦¬í¬íŠ¸ ì œëª©\n",
    "#     explorative=True,                  # ìì„¸í•œ ë¶„ì„ ëª¨ë“œ\n",
    "#     minimal=False                       # ìµœì†Œ ë¦¬í¬íŠ¸ ëª¨ë“œ í•´ì œ\n",
    "# )\n",
    "\n",
    "# # 3) ê²°ê³¼ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥\n",
    "# profile.to_file(\"data_report.html\")\n",
    "\n",
    "from pycaret.regression import (\n",
    "    setup, compare_models, tune_model,\n",
    "    finalize_model, predict_model, save_model\n",
    ")\n",
    "\n",
    "os.makedirs('./models_pycaret', exist_ok=True)\n",
    "final_preds = []\n",
    "\n",
    "for line in sorted(df['Line'].unique()):\n",
    "    if line!=7: continue \n",
    "    print(f\"\\nâ–¶â–¶ Line {line} AutoML ì‹œì‘\")\n",
    "\n",
    "    # 4-1) Line subset\n",
    "    train_line = df[df['Line']==line].copy()\n",
    "    test_line  = test[test['Line']==line].copy()\n",
    "    train_line['Congestion'] = train_line['Congestion'].astype(int)\n",
    "    # 4-2) PyCaret setup\n",
    "    exp = setup(\n",
    "        data=train_line,\n",
    "        target='Congestion',\n",
    "        session_id=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 4-3) ëª¨ë¸ ë¹„êµ & ì„ íƒ\n",
    "    best      = compare_models(verbose=1)           # ê¸°ë³¸ êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ\n",
    "    compare_results = pull()             # compare_models ê²°ê³¼ DataFrame\n",
    "    print(compare_results)\n",
    "    save_model(best, f'./models_pycaret/pycaret_base_line{line}')\n",
    "    \n",
    "    tuned     = tune_model(best, verbose=1)           # ì„ íƒëœ ëª¨ë¸ í•˜ì´í¼íŠœë‹\n",
    "    tune_results = pull()                # tune_model ê²°ê³¼ DataFrame\n",
    "    print(tune_results)\n",
    "    final_mod = finalize_model(tuned)      # íŠœë‹ëœ ëª¨ë¸ íŒŒì´ë„ë¼ì´ì¦ˆ\n",
    "    \n",
    "    # 4-4) í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡\n",
    "    preds = predict_model(final_mod, data=test_line)\n",
    "    print(pull())\n",
    "    # PyCaret íšŒê·€ì˜ ê²½ìš° ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì»¬ëŸ¼ëª… 'Label'ì— ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.\n",
    "    preds = preds.rename(columns={'Label':'Congestion_pred'}) \\\n",
    "                 [['hour','Line','station_number','Congestion_pred']]\n",
    "    preds['Congestion_pred'] = preds['Congestion_pred'].astype(int)\n",
    "    final_preds.append(preds)\n",
    "    \n",
    "    # 4-5) ëª¨ë¸ ì €ì¥\n",
    "    save_model(final_mod, f'./models_pycaret/pycaret_tuned_line{line}')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) ê²°ê³¼ í†µí•© ë° ì €ì¥\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "submission = pd.concat(final_preds).reset_index(drop=True)\n",
    "submission = submission[['hour','Line','station_number','Congestion_pred']] \\\n",
    "             .rename(columns={'Congestion_pred':'Congestion'})\n",
    "\n",
    "submission.to_csv('./test/250206_pycaret_submission.csv',\n",
    "                  index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\nâœ… AutoML ì™„ë£Œ, ì œì¶œ íŒŒì¼ ìƒì„±: ./test/250206_pycaret_submission.csv\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from holidayskr import year_holidays\n",
    "\n",
    "def preprocessing(data, t, address):\n",
    "    # 1) TM â†’ datetime, ì •ë ¬\n",
    "    data['TM'] = data['TM'].astype(str)\n",
    "    data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H')\n",
    "\n",
    "    # 2) ë²”ì£¼í˜• ë³€í™˜\n",
    "    cat_columns = ['Line', 'station_number', 'STN', 'station_name', 'Direction']\n",
    "    for col in cat_columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "    # 3) ê²°ì¸¡ê°’ ì²˜ë¦¬ìš© placeholder\n",
    "    data['WD'] = data['WD'].where(data['WD'] >= 0, np.nan)\n",
    "    data['WS'] = data['WS'].replace(-99.0, np.nan)\n",
    "    data['RN_DAY'] = data['RN_DAY'].replace(-99.0, np.nan)\n",
    "    data['RN_HR1'] = data['RN_HR1'].replace(-99.0, np.nan)\n",
    "    data['TA'] = data['TA'].replace(-99.0, np.nan)\n",
    "    data['ta_chi'] = data['ta_chi'].replace(-99.0, np.nan)\n",
    "    data['SI'] = data['SI'].replace(-99.0, np.nan)\n",
    "    data['HM'] = data['HM'].replace(-99.0, np.nan)\n",
    "\n",
    "    # 4) ì´ì§„ í”Œë˜ê·¸í™”\n",
    "    data['SI'] = data['SI'].notna().astype(int)\n",
    "\n",
    "    # 5) station_name ì¼ë¶€ êµì •\n",
    "    data['station_name'] = data['station_name'].astype(str).replace({\n",
    "        'ë‹¹ê³ ê°œ': 'ë¶ˆì•”ì‚°',\n",
    "        'ìì–‘(ëšì„¬í•œê°•ê³µì›)': 'ìì–‘',\n",
    "        'ì‹ ì´Œ(ì§€í•˜)': 'ì‹ ì´Œ'\n",
    "    })\n",
    "\n",
    "    # 6) ì™¸ë¶€ í…Œì´ë¸” ë³‘í•©\n",
    "    data = pd.merge(data, t, on=['Line','station_name'], how='left')\n",
    "    data['transfer'] = data['transfer'].fillna(0).astype(int)\n",
    "\n",
    "    data = pd.merge(data, address, on=['station_name'], how='left')\n",
    "\n",
    "    # 7) í‚¤, ì‹œê°„ íŒŒìƒ\n",
    "    data['key']   = data['Line'].astype(str) + '_' + data['station_name'].astype(str) + '_' + data['Direction'].astype(str)\n",
    "    data['year']  = data['TM'].dt.year - 2021\n",
    "    data['month'] = data['TM'].dt.month\n",
    "    data['day']   = data['TM'].dt.day\n",
    "    data['hour']  = data['TM'].dt.hour\n",
    "    data['weekday']      = data['TM'].dt.weekday\n",
    "    data['week_of_month']= (data['TM'].dt.day.sub(1) // 7) + 1\n",
    "    data['week_of_year'] = data['TM'].dt.isocalendar().week.astype(int)\n",
    "    data['day_of_year']  = data['TM'].dt.dayofyear\n",
    "\n",
    "    # 8) ê³µíœ´ì¼ í”Œë˜ê·¸\n",
    "    holidays = []\n",
    "    for yr in [2021, 2022, 2023, 2024]:\n",
    "        holidays += [d for d, _ in year_holidays(yr)]\n",
    "    data['date'] = data['TM'].dt.date\n",
    "    data['is_holiday'] = data['date'].isin(holidays).astype(int)\n",
    "    data['is_day_before_holiday'] = data['date'].shift(-1).isin(holidays).astype(int)\n",
    "    data['is_day_after_holiday']  = data['date'].shift(1).isin(holidays).astype(int)\n",
    "    data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    # 9) ì£¼ë§ í”Œë˜ê·¸\n",
    "    data['is_weekend'] = data['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "    # 10) ì‹œê°„ëŒ€ ë²”ì£¼í™”\n",
    "    data['time_period'] = np.where(data['hour'].isin([7,8,9]), 'ì¶œê·¼',\n",
    "                             np.where(data['hour'].isin([17,18,19]), 'í‡´ê·¼',\n",
    "                             np.where((data['hour']>9)&(data['hour']<17), 'ë‚®',\n",
    "                             np.where((data['hour']>19)&(data['hour']<21), 'ì €ë…',\n",
    "                             'ë°¤'))))\n",
    "\n",
    "    # ìˆœì„œí˜• ë²”ì£¼ ì¸ì½”ë”©\n",
    "    direction_order   = ['ìƒì„ ','í•˜ì„ ','ì™¸ì„ ','ë‚´ì„ ']\n",
    "    time_period_order = ['ë°¤','ì¶œê·¼','ë‚®','ì €ë…','í‡´ê·¼']\n",
    "    data['Direction']   = data['Direction'].astype(\n",
    "        CategoricalDtype(categories=direction_order, ordered=True)\n",
    "    ).cat.codes\n",
    "    data['time_period'] = data['time_period'].astype(\n",
    "        CategoricalDtype(categories=time_period_order, ordered=True)\n",
    "    ).cat.codes\n",
    "\n",
    "    # 11) ì£¼ê¸°ì„± ë³€ìˆ˜ (sin/cos)\n",
    "    data['sin_hod'] = np.sin(data['hour'] * (2*np.pi/24))\n",
    "    data['cos_hod'] = np.cos(data['hour'] * (2*np.pi/24))\n",
    "    data['sin_dow'] = np.sin(data['weekday'] * (2*np.pi/7))\n",
    "    data['cos_dow'] = np.cos(data['weekday'] * (2*np.pi/7))\n",
    "    # data['sin_dom'] = np.sin(data['day'] * (2*np.pi/31))\n",
    "    # data['cos_dom'] = np.cos(data['day'] * (2*np.pi/31))\n",
    "    # data['sin_wom'] = np.sin(data['week_of_month'] * (2*np.pi/5))\n",
    "    # data['cos_wom'] = np.cos(data['week_of_month'] * (2*np.pi/5))\n",
    "    # data['sin_woy'] = np.sin(data['week_of_year'] * (2*np.pi/52))\n",
    "    # data['cos_woy'] = np.cos(data['week_of_year'] * (2*np.pi/52))\n",
    "    data['sin_doy'] = np.sin(data['day_of_year'] * (2*np.pi/365))\n",
    "    data['cos_doy'] = np.cos(data['day_of_year'] * (2*np.pi/365))\n",
    "\n",
    "\n",
    "    # 12) ì„ í˜• ë³´ê°„\n",
    "    cols_to_fill = ['WD','RN_DAY','RN_HR1','TA','ta_chi','SI','HM','WS']\n",
    "    data[cols_to_fill] = data[cols_to_fill].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    print('ë³´ê°„ í›„ ë‚¨ì€ ê²°ì¸¡ê°’:\\n', data[cols_to_fill].isna().sum())\n",
    "    return data\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "df_processed  = preprocessing(df,  t, address)\n",
    "test_processed = preprocessing(test, t, address)\n",
    "print('ì™„ë£Œ')\n",
    "\n",
    "# â–¶ ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡\n",
    "ordered_cols = ['Direction', 'time_period']\n",
    "cat_cols = ['Line', 'address', 'station_name']\n",
    "num_cols = [\n",
    "    'HM', 'RN_DAY', 'RN_HR1', 'SI', 'STN', 'TA', 'WD', 'WS',\n",
    "    'cos_dom', 'cos_dow', 'cos_doy', 'cos_hod', 'cos_wom', 'cos_woy', 'day', 'day_of_year',\n",
    "    'hour', 'is_day_after_holiday', 'is_day_before_holiday', 'is_holiday', 'is_weekend',\n",
    "    'month', 'sin_dom', 'sin_dow', 'sin_doy', 'sin_hod', 'sin_wom', 'sin_woy',\n",
    "    'ta_chi', 'transfer', 'week_of_month', 'week_of_year', 'weekday', 'year','station_number'\n",
    "]\n",
    "cat_cols = cat_cols + ordered_cols\n",
    "features = num_cols + ordered_cols + cat_cols + ['Congestion']\n",
    "\n",
    "# â–¶ ì—°ë„ë³„ ë¶„ë¦¬\n",
    "# train_df = pd.concat([df[df['year'] == 0], df[df['year'] == 1]])[features]\n",
    "# val_df = df[df['year'] == 2][features]\n",
    "\n",
    "# # â–¶ category ì¸ì½”ë”©\n",
    "# for col in ['Line', 'station_name', 'address']:\n",
    "#     train_df[col] = train_df[col].astype('category')\n",
    "#     val_df[col] = val_df[col].astype('category')\n",
    "#     val_df[col] = val_df[col].cat.set_categories(train_df[col].cat.categories)\n",
    "#     train_df[col] = train_df[col].cat.codes\n",
    "#     val_df[col] = val_df[col].cat.codes\n",
    "\n",
    "# X_train = train_df.drop(columns=['Congestion','STN'])\n",
    "# X_val = val_df.drop(columns=['Congestion','STN'])\n",
    "\n",
    "# y_train = train_df['Congestion'].values\n",
    "# y_val = val_df['Congestion'].values\n",
    "\n",
    "# # â–¶ ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡\n",
    "# ordered_cols = ['Direction', 'time_period']\n",
    "# cat_cols = ['Line', 'address', 'station_name']\n",
    "# num_cols = [\n",
    "#     'HM', 'RN_DAY', 'RN_HR1', 'SI', 'STN', 'TA', 'WD', 'WS', 'Station_number',\n",
    "#     'cos_dom', 'cos_dow', 'cos_doy', 'cos_hod', 'cos_wom', 'cos_woy', 'day', 'day_of_year',\n",
    "#     'hour', 'is_day_after_holiday', 'is_day_before_holiday', 'is_holiday', 'is_weekend',\n",
    "#     'month', 'sin_dom', 'sin_dow', 'sin_doy', 'sin_hod', 'sin_wom', 'sin_woy',\n",
    "#     'ta_chi', 'transfer', 'week_of_month', 'week_of_year', 'weekday', 'year'\n",
    "# ]\n",
    "\n",
    "# features = num_cols + ordered_cols + cat_cols + ['Congestion']\n",
    "# df = df[features]\n",
    "\n",
    "###### ì „ì²˜ë¦¬\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ëª¨ë¸\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "results = []\n",
    "final_results = []\n",
    "\n",
    "# ê²½ê³  ì œê±° (ì„ íƒ)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# feature & target ì¶”ì¶œ\n",
    "feature_cols = features\n",
    "target_col = 'Congestion'\n",
    "\n",
    "# ğŸš‡ Lineë³„ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "for line in sorted(df_processed['Line'].unique()):\n",
    "    print(f\"\\nğŸ“˜ [Line {line}] ëª¨ë¸ í•™ìŠµ ì‹œì‘\")\n",
    "   # if line!=8: continue\n",
    "    df_line = df_processed[df_processed['Line'] == line].copy()\n",
    "    test_line = test_processed[test_processed['Line'] == line].copy()\n",
    "\n",
    "    # ë²”ì£¼í˜• ì²˜ë¦¬\n",
    "    for col in cat_cols:\n",
    "        df_line[col] = df_line[col].astype('category')\n",
    "        test_line[col] = test_line[col].astype('category')\n",
    "\n",
    "    # âœ… hour ê¸°ì¤€ ì •ë ¬ (í•µì‹¬)\n",
    "    df_line = df_line.sort_values('TM')\n",
    "\n",
    "    \n",
    "    X = df_line[feature_cols]\n",
    "    y = df_line[target_col].astype(int)\n",
    "    X_test = test_line[feature_cols]\n",
    "\n",
    "    # ì¸ì½”ë”©\n",
    "    if line ==8:\n",
    "        # 1ï¸âƒ£ ì›-í•« ì¸ì½”ë”©\n",
    "        X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "        X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "        \n",
    "        # 2ï¸âƒ£ ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (ë‘˜ ë‹¤ì—ì„œ í™•ì‹¤íˆ ì œê±°)\n",
    "        X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
    "        X_test_enc = X_test_enc.loc[:, ~X_test_enc.columns.duplicated()]\n",
    "        \n",
    "        # 3ï¸âƒ£ í…ŒìŠ¤íŠ¸ì…‹ ì»¬ëŸ¼ì„ í•™ìŠµì…‹ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "        X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "        X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "        X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "\n",
    "    # ì •ê·œí™”\n",
    "    mm = MinMaxScaler()\n",
    "    X_scaled = mm.fit_transform(X_enc)\n",
    "    X_test_scaled = mm.transform(X_test_enc)\n",
    "\n",
    "    # âœ… hour ìˆœ ë¶„í•  (train:val = 8:2)\n",
    "    split_idx = int(len(X_scaled) * 0.8)\n",
    "    X_train = X_scaled[:split_idx]\n",
    "    X_val   = X_scaled[split_idx:]\n",
    "    y_train = y.values[:split_idx]\n",
    "    y_val   = y.values[split_idx:]\n",
    "\n",
    "    # ëª¨ë¸ ì •ì˜\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.8,\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "    # ì„±ëŠ¥ í‰ê°€\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    r2 = r2_score(y_val, val_pred)\n",
    "    print(f\"âœ… RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
    "    results.append({'Line': line, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = np.round(model.predict(X_test_scaled)).astype(int)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    temp = test_line[['hour', 'Line', 'station_number']].copy()\n",
    "    temp['ì˜ˆì¸¡í˜¼ì¡ë„'] = y_pred\n",
    "    final_results.append(temp)\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    save_dir= 'models'\n",
    "    model_path = f\"./{save_dir}/xgb_line{line}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "test_processed\n",
    "# ğŸ” ê²°ê³¼ í†µí•© ë° ì €ì¥\n",
    "final_df = pd.concat(final_results)\n",
    "output_df = final_df[['ì˜ˆì¸¡í˜¼ì¡ë„']].rename(columns={'ì˜ˆì¸¡í˜¼ì¡ë„': 'Congestion'})\n",
    "\n",
    "output_df.to_csv(\n",
    "    './test/best_model_result.csv',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# ğŸ“Š ì„±ëŠ¥ ìš”ì•½\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "gap = pd.read_csv('./test/best_model_result.csv')\n",
    "gap.shape\n",
    "\n",
    "import os\n",
    "ì œì¶œ = pd.read_csv('./test/250206.csv')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ë§ˆìŠ¤í¬ ìƒì„±\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = mean_squared_error(\n",
    "    gap['Congestion'],\n",
    "    sub['Congestion'],\n",
    "    squared=False      # squared=False í•˜ë©´ RMSE ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¤Œ\n",
    ")\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "ì œì¶œ.describe().round()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
