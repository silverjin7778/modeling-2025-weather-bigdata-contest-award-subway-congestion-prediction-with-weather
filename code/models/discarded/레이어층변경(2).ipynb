{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0556a30-7651-4238-b9bb-b331bbba1b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\campus4D035\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# ğŸ“ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_parquet('prepro_data.parquet')\n",
    "df1 = df[df['í˜¸ì„ '] == 2].copy()\n",
    "cols = ['ì—°ë„', 'í˜¸ì„ ','ìƒí•˜êµ¬ë¶„','AWSì§€ì ì½”ë“œ']\n",
    "for col in cols:\n",
    "    df1[col] = df1[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e690baa9-c4e1-4b76-937d-ba7205153e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15187582 entries, 0 to 15653609\n",
      "Data columns (total 25 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ì‹œê°„        datetime64[ns]\n",
      " 1   í˜¸ì„         int64         \n",
      " 2   ì—­ë²ˆí˜¸       int32         \n",
      " 3   ì—­ëª…        category      \n",
      " 4   ìƒí•˜êµ¬ë¶„      category      \n",
      " 5   AWSì§€ì ì½”ë“œ   int64         \n",
      " 6   ê¸°ì˜¨        float32       \n",
      " 7   í’í–¥        float32       \n",
      " 8   í’ì†        float32       \n",
      " 9   ì¼ê°•ìˆ˜ëŸ‰      float32       \n",
      " 10  ì‹œê°„ê°•ìˆ˜ëŸ‰     float32       \n",
      " 11  ìƒëŒ€ìŠµë„      float32       \n",
      " 12  ì²´ê°ì˜¨ë„      float32       \n",
      " 13  í˜¼ì¡ë„       float32       \n",
      " 14  ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜    float32       \n",
      " 15  í•˜ì°¨ì´ìŠ¹ê°ìˆ˜    float32       \n",
      " 16  ë¯¸ì„¸ë¨¼ì§€      float32       \n",
      " 17  ì—°ë„        int32         \n",
      " 18  ì›”         int32         \n",
      " 19  ì¼         int32         \n",
      " 20  ì‹œ         int32         \n",
      " 21  ìš”ì¼        int32         \n",
      " 22  ì£¼ë§        int32         \n",
      " 23  ê³µíœ´ì¼       int32         \n",
      " 24  ì¼ì‚¬ëŸ‰_ì¸¡ì •ì—¬ë¶€  int32         \n",
      "dtypes: category(2), datetime64[ns](1), float32(11), int32(9), int64(2)\n",
      "memory usage: 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706b3bb1-27df-48c4-a2a1-a199069ace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1868722 entries, 950460 to 12006245\n",
      "Data columns (total 27 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ì‹œê°„        datetime64[ns]\n",
      " 1   í˜¸ì„         category      \n",
      " 2   ì—­ë²ˆí˜¸       int32         \n",
      " 3   ì—­ëª…        category      \n",
      " 4   ìƒí•˜êµ¬ë¶„      category      \n",
      " 5   AWSì§€ì ì½”ë“œ   category      \n",
      " 6   ê¸°ì˜¨        float32       \n",
      " 7   í’í–¥        float32       \n",
      " 8   í’ì†        float32       \n",
      " 9   ì¼ê°•ìˆ˜ëŸ‰      float32       \n",
      " 10  ì‹œê°„ê°•ìˆ˜ëŸ‰     float32       \n",
      " 11  ìƒëŒ€ìŠµë„      float32       \n",
      " 12  ì²´ê°ì˜¨ë„      float32       \n",
      " 13  í˜¼ì¡ë„       float32       \n",
      " 14  ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜    float32       \n",
      " 15  í•˜ì°¨ì´ìŠ¹ê°ìˆ˜    float32       \n",
      " 16  ë¯¸ì„¸ë¨¼ì§€      float32       \n",
      " 17  ì—°ë„        category      \n",
      " 18  ì›”         int32         \n",
      " 19  ì¼         int32         \n",
      " 20  ì£¼ë§        int32         \n",
      " 21  ê³µíœ´ì¼       int32         \n",
      " 22  ì¼ì‚¬ëŸ‰_ì¸¡ì •ì—¬ë¶€  int32         \n",
      " 23  ì‹œ_sin     float32       \n",
      " 24  ì‹œ_cos     float32       \n",
      " 25  ìš”ì¼_sin    float32       \n",
      " 26  ìš”ì¼_cos    float32       \n",
      "dtypes: category(5), datetime64[ns](1), float32(15), int32(6)\n",
      "memory usage: 188.9 MB\n"
     ]
    }
   ],
   "source": [
    "# ì‹œ (0~23 ê¸°ì¤€)\n",
    "df1['ì‹œ_sin'] = np.sin(2 * np.pi * df1['ì‹œ'] / 24)\n",
    "df1['ì‹œ_cos'] = np.cos(2 * np.pi * df1['ì‹œ'] / 24)\n",
    "\n",
    "# ìš”ì¼ (0~6 ê¸°ì¤€, ì›”~ì¼)\n",
    "df1['ìš”ì¼_sin'] = np.sin(2 * np.pi * df1['ìš”ì¼'] / 7)\n",
    "df1['ìš”ì¼_cos'] = np.cos(2 * np.pi * df1['ìš”ì¼'] / 7)\n",
    "\n",
    "# ì›”, ì¼ (1~12, 1~31 ê¸°ì¤€)\n",
    "le = LabelEncoder()\n",
    "df['ì›”'] = le.fit_transform(df['ì›”'])\n",
    "df['ì¼'] = le.fit_transform(df['ì¼'])\n",
    "\n",
    "# 1. float64 â†’ float32\n",
    "float_cols = df1.select_dtypes(include=['float64']).columns\n",
    "df1[float_cols] = df1[float_cols].astype('float32')\n",
    "\n",
    "# 2. int64 â†’ int32\n",
    "int_cols = df1.select_dtypes(include=['int64']).columns\n",
    "df1[int_cols] = df1[int_cols].astype('int32')\n",
    "\n",
    "# 3. object(ë¬¸ìì—´) â†’ category\n",
    "obj_cols = df1.select_dtypes(include=['object']).columns\n",
    "df1[obj_cols] = df1[obj_cols].astype('category')    \n",
    "    \n",
    "df1 = df1.drop(['ì‹œ', 'ìš”ì¼'], axis=1)\n",
    "df1.info()\n",
    "\n",
    "# âœ… [1] í”¼ì²˜ êµ¬ì„± ë° ì „ì²˜ë¦¬ --------------------------------------------------------\n",
    "\n",
    "# ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ 'í˜¼ì¡ë„' ì œê±°í•˜ì—¬ X ìƒì„±\n",
    "X = df1.drop(['í˜¼ì¡ë„', 'ì‹œê°„'], axis=1)\n",
    "\n",
    "# ğŸ”¤ ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”© (ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ëŠ” drop)\n",
    "o_h_e_x = pd.get_dummies(X, columns=['ì—°ë„', 'ì—­ëª…', 'í˜¸ì„ ', 'ìƒí•˜êµ¬ë¶„', 'AWSì§€ì ì½”ë“œ'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0491e526-a0af-418e-b4d0-21613ac7f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¢ X ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§)\n",
    "mm = MinMaxScaler()\n",
    "mm_x = mm.fit_transform(o_h_e_x)\n",
    "\n",
    "# ğŸ¯ y ì •ê·œí™” (í‘œì¤€í™”: í‰ê·  0, í‘œì¤€í¸ì°¨ 1)\n",
    "y = df1[['í˜¼ì¡ë„']]  # 2Dë¡œ ìœ ì§€\n",
    "ss = StandardScaler()\n",
    "ss_y = ss.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298565c2-6c8f-4ecf-bb8a-e18cc7256c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… [2] ì‹œê³„ì—´ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  --------------------------------------------------\n",
    "\n",
    "# ì „ì²´ì˜ 80%ë¥¼ í•™ìŠµìš©, ë‚˜ë¨¸ì§€ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©\n",
    "split_index = int(len(mm_x) * 0.8)\n",
    "\n",
    "train_x = mm_x[:split_index]\n",
    "train_y = ss_y[:split_index]\n",
    "\n",
    "val_x = mm_x[split_index:]\n",
    "val_y = ss_y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90ae72-2e1e-4e78-9d0c-45dd579c8c80",
   "metadata": {},
   "source": [
    "# (3) Attention Layer ì¶”ê°€ (Self-Attention ë˜ëŠ” Bahdanau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37dde7-5b7d-4160-a41d-3981fad4eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… [3] ì‹œê³„ì—´ ë°ì´í„° ìƒì„± -----------------------------------------------------------\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# âœ… ModelCheckpoint ì½œë°± ì¶”ê°€ ---------------------------------------\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_Attention_layer.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ê³¼ê±° 21ì‹œê°„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "sequence_length = 21\n",
    "batch_size = 32\n",
    "\n",
    "# TimeseriesGeneratorë¥¼ í†µí•´ ì‹œê³„ì—´ í•™ìŠµ/ê²€ì¦ì…‹ ìƒì„±\n",
    "train_gen = TimeseriesGenerator(train_x, train_y, length=sequence_length, batch_size=batch_size)\n",
    "val_gen   = TimeseriesGenerator(val_x, val_y, length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "# âœ… [4] Attention ê¸°ë°˜ LSTM ëª¨ë¸ ì •ì˜ --------------------------------------------------------\n",
    "\n",
    "# ì»¤ìŠ¤í…€ Attention Layer ì •ì˜\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, time_steps, hidden_size)\n",
    "        score = tf.nn.softmax(tf.keras.backend.sum(inputs, axis=2), axis=1)\n",
    "        context = tf.keras.backend.batch_dot(score, inputs, axes=1)\n",
    "        return context\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° ì •ì˜ (Functional API ì‚¬ìš©)\n",
    "input_layer = Input(shape=(sequence_length, train_x.shape[1]))\n",
    "lstm = LSTM(64, return_sequences=True)(input_layer)\n",
    "attn = Attention()(lstm)\n",
    "flatten = Flatten()(attn)\n",
    "output = Dense(1)(flatten)\n",
    "\n",
    "model3 = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model3.compile(loss='mse', optimizer=Adam(0.001))\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "history3 = model3.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        # early_stop, \n",
    "        reduce_lr,\n",
    "        checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c928b8c-6580-4949-8181-34615ab22858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ì‹œê°í™”\n",
    "plt.rcParams['font.family']='Malgun Gothic'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history3.history['loss'], label='Train Loss')\n",
    "plt.plot(history3.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Attention Layer ì¶”ê°€ Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f47e2-1d3d-4db1-8fce-dc6d184fde2c",
   "metadata": {},
   "source": [
    "# (4) Layer Normalization ë˜ëŠ” BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf7db7-c925-4926-95d4-67d5ca1073e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… [3] ì‹œê³„ì—´ ë°ì´í„° ìƒì„± -----------------------------------------------------------\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# âœ… ModelCheckpoint ì½œë°± ì¶”ê°€ ---------------------------------------\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_normalizaion.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ê³¼ê±° 21ì‹œê°„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "sequence_length = 21\n",
    "batch_size = 32\n",
    "\n",
    "# TimeseriesGeneratorë¥¼ í†µí•´ ì‹œê³„ì—´ í•™ìŠµ/ê²€ì¦ì…‹ ìƒì„±\n",
    "train_gen = TimeseriesGenerator(train_x, train_y, length=sequence_length, batch_size=batch_size)\n",
    "val_gen   = TimeseriesGenerator(val_x, val_y, length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "# âœ… [4] Attention ê¸°ë°˜ LSTM ëª¨ë¸ ì •ì˜ --------------------------------------------------------\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "model4 = Sequential([\n",
    "    Input(shape=(sequence_length, train_x.shape[1])),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model4.compile(loss='mse', optimizer=Adam(0.001))\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "history4 = model4.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        # early_stop, \n",
    "        reduce_lr,\n",
    "        checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac56e7-93a8-4a69-8952-12a45525fa46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ì‹œê°í™”\n",
    "plt.rcParams['font.family']='Malgun Gothic'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history4.history['loss'], label='Train Loss')\n",
    "plt.plot(history4.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Layer Normalization ì¶”ê°€ Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4da16f-d25a-4772-b5db-5f5aa69410db",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486fc3b-02b3-49ca-ae97-d0c850d7ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… [5] ì˜ˆì¸¡ ë° í‰ê°€ ---------------------------------------------------------------\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model_3 = load_model('best_model_Attention_layer.h5')\n",
    "loaded_model_4 = load_model('best_model_normalizaion.h5')\n",
    "\n",
    "# ê²€ì¦ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "pred_scaled_3 = loaded_model_3.predict(val_gen)\n",
    "pred_scaled_4 = loaded_model_4.predict(val_gen)\n",
    "\n",
    "# ì •ê·œí™” í•´ì œ (ì—­ì •ê·œí™”) â†’ ì‹¤ì œ í˜¼ì¡ë„ ë‹¨ìœ„ë¡œ ë³µì›\n",
    "pred_3 = ss.inverse_transform(pred_scaled_3)\n",
    "pred_4 = ss.inverse_transform(pred_scaled_4)\n",
    "\n",
    "# ì‹¤ì œ yë„ ì‹œê³„ì—´ ìŠ¬ë¼ì´ì‹± ê¸°ì¤€ì— ë§ê²Œ ìë¥´ê¸°\n",
    "true = ss.inverse_transform(val_y[sequence_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4358d400-c1a5-40a3-98fc-26842c5a636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…batch size = 32\n",
      " ê²€ì¦ RMSE: 4.0428\n",
      " RÂ² Score: 0.9709\n",
      "\n",
      "âœ…batch size = 64\n",
      " ê²€ì¦ RMSE: 4.6141\n",
      " RÂ² Score: 0.9620\n",
      "\n",
      "âœ…batch size = 128\n",
      " ê²€ì¦ RMSE: 4.8596\n",
      " RÂ² Score: 0.9579\n",
      "\n",
      "âœ…batch size = 256\n",
      " ê²€ì¦ RMSE: 4.6932\n",
      " RÂ² Score: 0.9607\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ì§€í‘œ ê³„ì‚°                            # RÂ² Score\n",
    "rmse_3 = mean_squared_error(true, pred_3, squared=False)  # RMSE\n",
    "r2_3 = r2_score(true, pred_3)                             # RÂ² Score\n",
    "rmse_4 = mean_squared_error(true, pred_4, squared=False)  # RMSE\n",
    "r2_4 = r2_score(true, pred_4)                             # RÂ² Score\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print('âœ…Attention Layer ì¶”ê°€')\n",
    "print(f\" ê²€ì¦ RMSE: {rmse_3:.4f}\")\n",
    "print(f\" RÂ² Score: {r2_3:.4f}\")\n",
    "print()\n",
    "print('âœ…Layer Normalization')\n",
    "print(f\" ê²€ì¦ RMSE: {rmse_4:.4f}\")\n",
    "print(f\" RÂ² Score: {r2_4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1cac1-23a9-4c31-bd8f-4a6f234e9055",
   "metadata": {},
   "source": [
    "Epoch 23: val_loss improved from 0.03989 to 0.03952, saving model to best_model_batchsize_32.h5  \n",
    "Epoch 15: val_loss improved from 0.05258 to 0.05148, saving model to best_model_batchsize_64.h5  \n",
    "Epoch 19: val_loss improved from 0.05784 to 0.05711, saving model to best_model_batchsize_128.h5  \n",
    "Epoch 29: val_loss improved from 0.05363 to 0.05326, saving model to best_model_batchsize_256.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842e46d-c238-4313-bd99-a2d052bbbcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
