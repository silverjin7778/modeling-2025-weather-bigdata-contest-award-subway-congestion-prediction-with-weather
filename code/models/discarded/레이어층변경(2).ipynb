{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0556a30-7651-4238-b9bb-b331bbba1b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\campus4D035\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 📦 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# 📁 데이터 불러오기\n",
    "df = pd.read_parquet('prepro_data.parquet')\n",
    "df1 = df[df['호선'] == 2].copy()\n",
    "cols = ['연도', '호선','상하구분','AWS지점코드']\n",
    "for col in cols:\n",
    "    df1[col] = df1[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e690baa9-c4e1-4b76-937d-ba7205153e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15187582 entries, 0 to 15653609\n",
      "Data columns (total 25 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   시간        datetime64[ns]\n",
      " 1   호선        int64         \n",
      " 2   역번호       int32         \n",
      " 3   역명        category      \n",
      " 4   상하구분      category      \n",
      " 5   AWS지점코드   int64         \n",
      " 6   기온        float32       \n",
      " 7   풍향        float32       \n",
      " 8   풍속        float32       \n",
      " 9   일강수량      float32       \n",
      " 10  시간강수량     float32       \n",
      " 11  상대습도      float32       \n",
      " 12  체감온도      float32       \n",
      " 13  혼잡도       float32       \n",
      " 14  승차총승객수    float32       \n",
      " 15  하차총승객수    float32       \n",
      " 16  미세먼지      float32       \n",
      " 17  연도        int32         \n",
      " 18  월         int32         \n",
      " 19  일         int32         \n",
      " 20  시         int32         \n",
      " 21  요일        int32         \n",
      " 22  주말        int32         \n",
      " 23  공휴일       int32         \n",
      " 24  일사량_측정여부  int32         \n",
      "dtypes: category(2), datetime64[ns](1), float32(11), int32(9), int64(2)\n",
      "memory usage: 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706b3bb1-27df-48c4-a2a1-a199069ace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1868722 entries, 950460 to 12006245\n",
      "Data columns (total 27 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   시간        datetime64[ns]\n",
      " 1   호선        category      \n",
      " 2   역번호       int32         \n",
      " 3   역명        category      \n",
      " 4   상하구분      category      \n",
      " 5   AWS지점코드   category      \n",
      " 6   기온        float32       \n",
      " 7   풍향        float32       \n",
      " 8   풍속        float32       \n",
      " 9   일강수량      float32       \n",
      " 10  시간강수량     float32       \n",
      " 11  상대습도      float32       \n",
      " 12  체감온도      float32       \n",
      " 13  혼잡도       float32       \n",
      " 14  승차총승객수    float32       \n",
      " 15  하차총승객수    float32       \n",
      " 16  미세먼지      float32       \n",
      " 17  연도        category      \n",
      " 18  월         int32         \n",
      " 19  일         int32         \n",
      " 20  주말        int32         \n",
      " 21  공휴일       int32         \n",
      " 22  일사량_측정여부  int32         \n",
      " 23  시_sin     float32       \n",
      " 24  시_cos     float32       \n",
      " 25  요일_sin    float32       \n",
      " 26  요일_cos    float32       \n",
      "dtypes: category(5), datetime64[ns](1), float32(15), int32(6)\n",
      "memory usage: 188.9 MB\n"
     ]
    }
   ],
   "source": [
    "# 시 (0~23 기준)\n",
    "df1['시_sin'] = np.sin(2 * np.pi * df1['시'] / 24)\n",
    "df1['시_cos'] = np.cos(2 * np.pi * df1['시'] / 24)\n",
    "\n",
    "# 요일 (0~6 기준, 월~일)\n",
    "df1['요일_sin'] = np.sin(2 * np.pi * df1['요일'] / 7)\n",
    "df1['요일_cos'] = np.cos(2 * np.pi * df1['요일'] / 7)\n",
    "\n",
    "# 월, 일 (1~12, 1~31 기준)\n",
    "le = LabelEncoder()\n",
    "df['월'] = le.fit_transform(df['월'])\n",
    "df['일'] = le.fit_transform(df['일'])\n",
    "\n",
    "# 1. float64 → float32\n",
    "float_cols = df1.select_dtypes(include=['float64']).columns\n",
    "df1[float_cols] = df1[float_cols].astype('float32')\n",
    "\n",
    "# 2. int64 → int32\n",
    "int_cols = df1.select_dtypes(include=['int64']).columns\n",
    "df1[int_cols] = df1[int_cols].astype('int32')\n",
    "\n",
    "# 3. object(문자열) → category\n",
    "obj_cols = df1.select_dtypes(include=['object']).columns\n",
    "df1[obj_cols] = df1[obj_cols].astype('category')    \n",
    "    \n",
    "df1 = df1.drop(['시', '요일'], axis=1)\n",
    "df1.info()\n",
    "\n",
    "# ✅ [1] 피처 구성 및 전처리 --------------------------------------------------------\n",
    "\n",
    "# 🎯 타겟 변수 '혼잡도' 제거하여 X 생성\n",
    "X = df1.drop(['혼잡도', '시간'], axis=1)\n",
    "\n",
    "# 🔤 범주형 변수 원-핫 인코딩 (첫 번째 카테고리는 drop)\n",
    "o_h_e_x = pd.get_dummies(X, columns=['연도', '역명', '호선', '상하구분', 'AWS지점코드'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0491e526-a0af-418e-b4d0-21613ac7f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔢 X 정규화 (0~1 범위로 스케일링)\n",
    "mm = MinMaxScaler()\n",
    "mm_x = mm.fit_transform(o_h_e_x)\n",
    "\n",
    "# 🎯 y 정규화 (표준화: 평균 0, 표준편차 1)\n",
    "y = df1[['혼잡도']]  # 2D로 유지\n",
    "ss = StandardScaler()\n",
    "ss_y = ss.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298565c2-6c8f-4ecf-bb8a-e18cc7256c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ [2] 시계열 학습/검증 데이터 분할 --------------------------------------------------\n",
    "\n",
    "# 전체의 80%를 학습용, 나머지 20%를 검증용으로 사용\n",
    "split_index = int(len(mm_x) * 0.8)\n",
    "\n",
    "train_x = mm_x[:split_index]\n",
    "train_y = ss_y[:split_index]\n",
    "\n",
    "val_x = mm_x[split_index:]\n",
    "val_y = ss_y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90ae72-2e1e-4e78-9d0c-45dd579c8c80",
   "metadata": {},
   "source": [
    "# (3) Attention Layer 추가 (Self-Attention 또는 Bahdanau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37dde7-5b7d-4160-a41d-3981fad4eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ [3] 시계열 데이터 생성 -----------------------------------------------------------\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# ✅ ModelCheckpoint 콜백 추가 ---------------------------------------\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_Attention_layer.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 과거 21시간 데이터를 기반으로 예측\n",
    "sequence_length = 21\n",
    "batch_size = 32\n",
    "\n",
    "# TimeseriesGenerator를 통해 시계열 학습/검증셋 생성\n",
    "train_gen = TimeseriesGenerator(train_x, train_y, length=sequence_length, batch_size=batch_size)\n",
    "val_gen   = TimeseriesGenerator(val_x, val_y, length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "# ✅ [4] Attention 기반 LSTM 모델 정의 --------------------------------------------------------\n",
    "\n",
    "# 커스텀 Attention Layer 정의\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, time_steps, hidden_size)\n",
    "        score = tf.nn.softmax(tf.keras.backend.sum(inputs, axis=2), axis=1)\n",
    "        context = tf.keras.backend.batch_dot(score, inputs, axes=1)\n",
    "        return context\n",
    "\n",
    "# 모델 구조 정의 (Functional API 사용)\n",
    "input_layer = Input(shape=(sequence_length, train_x.shape[1]))\n",
    "lstm = LSTM(64, return_sequences=True)(input_layer)\n",
    "attn = Attention()(lstm)\n",
    "flatten = Flatten()(attn)\n",
    "output = Dense(1)(flatten)\n",
    "\n",
    "model3 = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# 모델 컴파일\n",
    "model3.compile(loss='mse', optimizer=Adam(0.001))\n",
    "\n",
    "# 콜백 설정\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5)\n",
    "\n",
    "# 모델 학습\n",
    "history3 = model3.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        # early_stop, \n",
    "        reduce_lr,\n",
    "        checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c928b8c-6580-4949-8181-34615ab22858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 📊 학습 및 검증 손실 시각화\n",
    "plt.rcParams['font.family']='Malgun Gothic'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history3.history['loss'], label='Train Loss')\n",
    "plt.plot(history3.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Attention Layer 추가 Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f47e2-1d3d-4db1-8fce-dc6d184fde2c",
   "metadata": {},
   "source": [
    "# (4) Layer Normalization 또는 BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf7db7-c925-4926-95d4-67d5ca1073e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ [3] 시계열 데이터 생성 -----------------------------------------------------------\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# ✅ ModelCheckpoint 콜백 추가 ---------------------------------------\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_normalizaion.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 과거 21시간 데이터를 기반으로 예측\n",
    "sequence_length = 21\n",
    "batch_size = 32\n",
    "\n",
    "# TimeseriesGenerator를 통해 시계열 학습/검증셋 생성\n",
    "train_gen = TimeseriesGenerator(train_x, train_y, length=sequence_length, batch_size=batch_size)\n",
    "val_gen   = TimeseriesGenerator(val_x, val_y, length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "# ✅ [4] Attention 기반 LSTM 모델 정의 --------------------------------------------------------\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "model4 = Sequential([\n",
    "    Input(shape=(sequence_length, train_x.shape[1])),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "# 모델 컴파일\n",
    "model4.compile(loss='mse', optimizer=Adam(0.001))\n",
    "\n",
    "# 콜백 설정\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5)\n",
    "\n",
    "# 모델 학습\n",
    "history4 = model4.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        # early_stop, \n",
    "        reduce_lr,\n",
    "        checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac56e7-93a8-4a69-8952-12a45525fa46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 📊 학습 및 검증 손실 시각화\n",
    "plt.rcParams['font.family']='Malgun Gothic'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history4.history['loss'], label='Train Loss')\n",
    "plt.plot(history4.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Layer Normalization 추가 Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4da16f-d25a-4772-b5db-5f5aa69410db",
   "metadata": {},
   "source": [
    "# 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486fc3b-02b3-49ca-ae97-d0c850d7ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ [5] 예측 및 평가 ---------------------------------------------------------------\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model_3 = load_model('best_model_Attention_layer.h5')\n",
    "loaded_model_4 = load_model('best_model_normalizaion.h5')\n",
    "\n",
    "# 검증셋에 대해 예측 수행\n",
    "pred_scaled_3 = loaded_model_3.predict(val_gen)\n",
    "pred_scaled_4 = loaded_model_4.predict(val_gen)\n",
    "\n",
    "# 정규화 해제 (역정규화) → 실제 혼잡도 단위로 복원\n",
    "pred_3 = ss.inverse_transform(pred_scaled_3)\n",
    "pred_4 = ss.inverse_transform(pred_scaled_4)\n",
    "\n",
    "# 실제 y도 시계열 슬라이싱 기준에 맞게 자르기\n",
    "true = ss.inverse_transform(val_y[sequence_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4358d400-c1a5-40a3-98fc-26842c5a636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅batch size = 32\n",
      " 검증 RMSE: 4.0428\n",
      " R² Score: 0.9709\n",
      "\n",
      "✅batch size = 64\n",
      " 검증 RMSE: 4.6141\n",
      " R² Score: 0.9620\n",
      "\n",
      "✅batch size = 128\n",
      " 검증 RMSE: 4.8596\n",
      " R² Score: 0.9579\n",
      "\n",
      "✅batch size = 256\n",
      " 검증 RMSE: 4.6932\n",
      " R² Score: 0.9607\n"
     ]
    }
   ],
   "source": [
    "# 평가 지표 계산                            # R² Score\n",
    "rmse_3 = mean_squared_error(true, pred_3, squared=False)  # RMSE\n",
    "r2_3 = r2_score(true, pred_3)                             # R² Score\n",
    "rmse_4 = mean_squared_error(true, pred_4, squared=False)  # RMSE\n",
    "r2_4 = r2_score(true, pred_4)                             # R² Score\n",
    "\n",
    "# 결과 출력\n",
    "print('✅Attention Layer 추가')\n",
    "print(f\" 검증 RMSE: {rmse_3:.4f}\")\n",
    "print(f\" R² Score: {r2_3:.4f}\")\n",
    "print()\n",
    "print('✅Layer Normalization')\n",
    "print(f\" 검증 RMSE: {rmse_4:.4f}\")\n",
    "print(f\" R² Score: {r2_4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1cac1-23a9-4c31-bd8f-4a6f234e9055",
   "metadata": {},
   "source": [
    "Epoch 23: val_loss improved from 0.03989 to 0.03952, saving model to best_model_batchsize_32.h5  \n",
    "Epoch 15: val_loss improved from 0.05258 to 0.05148, saving model to best_model_batchsize_64.h5  \n",
    "Epoch 19: val_loss improved from 0.05784 to 0.05711, saving model to best_model_batchsize_128.h5  \n",
    "Epoch 29: val_loss improved from 0.05363 to 0.05326, saving model to best_model_batchsize_256.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842e46d-c238-4313-bd99-a2d052bbbcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
